{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opponent-degree",
   "metadata": {},
   "source": [
    "# Text Cleaning Before Document Vectorization | BAIS:6100\n",
    "\n",
    "**Instructor: Qihang Lin**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-knowing",
   "metadata": {},
   "source": [
    "When using vectorizers from **sklearn** to create a DTM, we have less control on how the documents should be cleaned before they are converted into a DTM. For example, a vectorizer from sklearn always does stemming after creating n-grams. As a result, only the last term in the n-grams will be stemmed. \n",
    "\n",
    "If we want to do something different, we can use text cleaning techniques and \"for-loop\" to clean each document before it is taken by vectorizers. This way, we can better control the process have more flexibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incorporated-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "df = pd.read_csv(\"classdata/clinton-street-social-club.csv\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electric-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the stop word list of nltk\n",
    "global_stopwords = nltk.corpus.stopwords.words(\"english\") \n",
    "#Creat the Snowball stemmer\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-contact",
   "metadata": {},
   "source": [
    "The following code clean each document without using vectorizer. We can put the cleaning steps in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "present-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temptext=[]                              #Create an empty list to receive the cleaned docs. \n",
    "for doc in df[\"reviews\"]:                #Let doc becomes each of the reviews sequentially.   \n",
    "  tokens = nltk.word_tokenize(doc)       #Tokenize a review\n",
    "  tokens = [s.lower() for s in tokens]   #Turn tokens to lower\n",
    "  tokens = [s for s in tokens if s not in global_stopwords] #Remove stop words\n",
    "  tokens = [s for s in tokens if len(s)>2]                  #Remove short tokens (<=2 char)\n",
    "  tokens = [stemmer.stem(s) for s in tokens]                #Stem each token\n",
    "  doc = \" \".join(tokens)                 #Join tokens with whitespace to create a cleaned review \n",
    "  temptext.append(doc)                   #Add it to the receiving list.\n",
    "df[\"reviews_clean\"]=temptext             #Create a new column with the cleaned reviews.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "covered-attack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"jazzi vibe chill atmospher clinton street social club may speakeasi iowa citi entranc small door next short burger barber shop n't know re look may never find serious mimic new orlean cultur delici food creativ alcohol beverag must tri beignet shrimp cocktail house-batter curd sweet corn fritter alcohol drink love ramo gin fizz tend like anyth tast alcohol one right balanc right amount fizz ad foam egg white uniqu compar usual bar locat downtown opportun learn tend bar know bartend joy girl definit know alcohol made delici drink told histor discoveri\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the first cleaned review.\n",
    "df[\"reviews_clean\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-national",
   "metadata": {},
   "source": [
    "Finally, we give the cleaned documents to the vectorizer. Since we have removed the stop words, we don't need to do it again during vectorization. This example uses the standard vectorizer. We can add optional functions (e.g. tfidf, customized vocabulary, n-gram) if we want. \n",
    "\n",
    "Here, we use bigram to create the DTM. You can compare it with the bigram DTM we created in \"Document Vectorization.ipynb\". At that document, we can only stem the second term in each bigram but here we can stem both terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "linear-dublin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 7082)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "DTM = vectorizer.fit_transform(df[\"reviews_clean\"])\n",
    "DTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe17241d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iowa citi</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clinton street</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chees curd</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social club</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>street social</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>happi hour</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>devil egg</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mac chees</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>feel like</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>one best</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Term  Frequency\n",
       "0       iowa citi         51\n",
       "1  clinton street         37\n",
       "2      chees curd         34\n",
       "3     social club         31\n",
       "4   street social         23\n",
       "5      happi hour         18\n",
       "6       devil egg         14\n",
       "7       mac chees         12\n",
       "8       feel like         11\n",
       "9        one best         10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Term': vectorizer.get_feature_names(),\n",
    "                   'Frequency': DTM.sum(axis=0).tolist()[0]\n",
    "                  })\n",
    "\n",
    "df.sort_values(by=\"Frequency\",inplace=True,ascending=False)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
