{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fea562ca",
   "metadata": {},
   "source": [
    "# Sparse Linear Model | BAIS:6100\n",
    "\n",
    "**Instructor: Qihang Lin**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd8ceb",
   "metadata": {},
   "source": [
    "## Predictive Analytics with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82865a37",
   "metadata": {},
   "source": [
    "Predict categorical outcome using text data:\n",
    "   - spam email / regular email\n",
    "   - news about sports / news about politics\n",
    "   - positive reviews / negative reviews\n",
    "   - true reviews / fake reviews\n",
    "   - useful reviews / useless reviews\n",
    "   - election result (through social media posts)\n",
    "\n",
    "Predict numeric outcome using text data:\n",
    "   - Star rating \n",
    "   - Box office revenue \n",
    "   - Stock return\n",
    "   - Demand of a product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e25e24",
   "metadata": {},
   "source": [
    "A predictive model can be built on the vectorized form, such as the DTM, of text data.\n",
    "\n",
    "* Sparse logistic/linear regression\n",
    "* Decision/regression tree\n",
    "* Random forest\n",
    "* Boosting\n",
    "* K-nearest neighbors\n",
    "* Naive Bayesian classifier\n",
    "* Support vector machine\n",
    "* Deep neural network\n",
    "* ......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca754b0",
   "metadata": {},
   "source": [
    "## Training VS Testing\n",
    "* Each predictive model can be evaluated by its **in-sample performance** and **out-of-sample performance**:\n",
    "    * **In-sample performance**: The prediction performance on the data points that were used to build the model.\n",
    "    * **Out-of-sample performance**: The prediction performance on the data points that were not involved in building the model (for example, the real-world instances coming in the future).\n",
    "    \n",
    "    \n",
    "* A **model validation** procedure is used to estimate a model's out-of-sample performance:\n",
    "    - Split our data into training and testing sets (e.g., 75% vs 25% or 80% vs 20%).\n",
    "    - Pretend that the testing set is the data in the future.\n",
    "    - Build the model only using the training set and then evaluate its perfomance on both the training and testing sets.\n",
    "    - Training performance $=$ in-sample performance \n",
    "    - Testing performance $\\approx$ out-of-sample performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b93bb",
   "metadata": {},
   "source": [
    "## Underfit VS Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c80bf",
   "metadata": {},
   "source": [
    "* Underfitting: the model is too simple to characterize the underlying truth.\n",
    "    - High error in both training and testing data.\n",
    "\n",
    "* Overfitting: the model is too complex so it \"memorizes\" the label of each document in the training set but ignores the underlying truth.\n",
    "    - Low error in training but high error in testing data.\n",
    "\n",
    "* The complexity of a model depends on the size of vocabulary and some hyper-parameters.\n",
    "    \n",
    "* Regularization and model validation can help us avoid overfitting\n",
    "\n",
    "<img src=https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/Screenshot-2020-02-06-at-11.09.13.png width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e78357",
   "metadata": {},
   "source": [
    "## Steps of Predictive Text Analytics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08574f54",
   "metadata": {},
   "source": [
    "An end-to-end predictive text analytics pipeline is composed of three main components:\n",
    "\n",
    "**1. Dataset Preparation:** Load a dataset and split it into training and testing sets (e.g. 80% and 20%). Then perform basic pre-processings, tokenization, words replacement, removing stop words, stemming, and etc.\n",
    "\n",
    "**2. Feature Engineering:** Create the DTM. Determine a vocabulary. TF VS TFIDF VS Binary. Words VS n-grams. Normalization or not. Whether to include other features like the length of document, POS-tag counts, word embedding, topic models, sentiment scores, and so on. \n",
    "\n",
    "**3. Model Training:** Train a machine learning model using the training set. Tune the **hyper-parameter**. Evaluate the performance of model on the testing set. \n",
    "\n",
    "Repeat Steps 1, 2 and 3 with different choices to improve the performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cab98ec",
   "metadata": {},
   "source": [
    "<img src=http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157440e7",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "* Vectorization of a document: $\\mathbf{x}=(x_1,x_2,\\dots,x_n)$ \n",
    "    * $\\mathbf{x}$ is a row in DTM and also called a feature vector.\n",
    "    \n",
    "* Each document has a class label, denoted by $y$.\n",
    "    * For example, $y=pos$ or $neg$ in binary classification.\n",
    "\n",
    "* Linear score: \n",
    "    * $\\alpha+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_nx_n$\n",
    "* Coefficients: \n",
    "    * Intercept: $\\alpha$ \n",
    "    * Slopes: $\\mathbf{\\beta}=(\\beta_1,\\beta_2,\\dots,\\beta_n)$\n",
    "* Logistic regression makes prediction based on the linear score:\n",
    "    * Predict $y$ as \"pos\" if $\\alpha+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_nx_n>0$ \n",
    "    * Predict $y$ as \"neg\" if $\\alpha+\\beta_1x_1+\\beta_2x_2+\\cdots+\\beta_nx_n<0$\n",
    "* Impact of terms: \n",
    "    * $\\beta_i>0$: A document with a high frequency in term $i$ will be positive. \n",
    "    * $\\beta_i<0$: A document with a high frequency in term $i$ will be negative.\n",
    "    * $\\beta_i=0$: Term $i$ has no impact on the class label $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f45a3",
   "metadata": {},
   "source": [
    "## Sparse Logistic Regression\n",
    "\n",
    "* A logistic regression (LR) model is very likely overfitted when the vocabulary is very large. \n",
    "\n",
    "\n",
    "* A sparse logistic regression (SLR) model is a LR model with slope $\\beta_i=0$ for terms with no significant impact. These terms are essentially removed from the model, which reduces the size of vocabulary and avoids overfitting.\n",
    "\n",
    "* The term \"sparse\" in SLR comes from the fact that the slopes $\\mathbf{\\beta}=(\\beta_1,\\beta_2,\\dots,\\beta_n)$ contain many zeros.\n",
    "\n",
    "\n",
    "* Theoretically, SLR can be applied with a vocabulary of any size with no worry of overfitting.\n",
    "\n",
    "\n",
    "* SLR searches for $\\mathbf{\\beta}$ that minimizes\n",
    "$$\n",
    "TrainingError + \\frac{1}{C}\\times\\|\\mathbf{\\beta}\\|_1 \n",
    "$$\n",
    "   - $\\|\\mathbf{\\beta}\\|_1=|\\beta_1|+|\\beta_2|+\\dots+|\\beta_n|$: L1 regularizer. $\\mathbf{\\beta}$ will be sparse if $\\|\\mathbf{\\beta}\\|_1$ is small. Therefore, we can make $\\mathbf{\\beta}$ sparse by minimizing $\\|\\mathbf{\\beta}\\|_1$.\n",
    "   - $C$ is a **regularization parameter** that balances between $TrainingError$ and $\\|\\mathbf{\\beta}\\|_1$ during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89137b0",
   "metadata": {},
   "source": [
    "## Sparsity and Regularization Parameter\n",
    "\n",
    "* Regularization parameter $C$ must be provided when building a sparse logistic regression model. \n",
    "* Small $C$:\n",
    "    * $\\mathbf{\\beta}$ becomes more sparse. SLR tends to be simple and underfitted.\n",
    "* Large $C$:\n",
    "    * $\\mathbf{\\beta}$ becomes more dense. SLR tends to be complex and overfitted.\n",
    "* $C$ must be tuned carefully to achieve the best performance of SLR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c557a0",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd0cd5",
   "metadata": {},
   "source": [
    "1. Clean the text data if needed. \n",
    "\n",
    "2. Split the text data into training and testing sets. \n",
    "\n",
    "3. Create DTMs for both sets. Make sure they have the same vocabulary and that vocabulary must be determined only based on the training set.\n",
    "  \n",
    "In principle, no information from the testing set should be used during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfa94f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment Polarity</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>egyd hr fhjfhjtjr rhjtjt tjfhjfwettert ryur yu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>I have never purchased a George Foreman grill ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>I ll sum it up in one word  SCAM  Those people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>When our ten year old Mr  Coffee stopped worki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>I purchased the Stanley Bostitch QuietSharp Ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment Polarity                                             Review\n",
       "0                neg  egyd hr fhjfhjtjr rhjtjt tjfhjfwettert ryur yu...\n",
       "1                neg  I have never purchased a George Foreman grill ...\n",
       "2                neg  I ll sum it up in one word  SCAM  Those people...\n",
       "3                pos  When our ten year old Mr  Coffee stopped worki...\n",
       "4                neg  I purchased the Stanley Bostitch QuietSharp Ex..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"classdata/Lies_and_Truths.csv\")\n",
    "df = df[df.Domain==\"Electronics\"].copy()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df=df[[\"Sentiment Polarity\",\"Review\"]]   #Keep only the columns we need.\n",
    "\n",
    "#Clean the text if needed.\n",
    "df[\"Review\"]=[re.sub(\"[^\\w\\s]|_\", \" \", s) for s in df[\"Review\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b84bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.33, random_state=2021)\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc28fc3e",
   "metadata": {},
   "source": [
    "Note that a random partition will make the analysis result not reproducible. That's why **random_state=2021** is set to make sure the partition is not random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a18af",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a27a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 3323)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk \n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\") \n",
    "\n",
    "vectorizer=StemmedTfidfVectorizer(stop_words=nltk_stopwords, norm=None)\n",
    "\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_train[\"Review\"])\n",
    "train_y = df_train[\"Sentiment Polarity\"]\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66d437",
   "metadata": {},
   "source": [
    "By applying **vectorizer.fit_transform** to df_train[\"Review\"] above, the vocabulary of train_x are learned from the training data and saved internally. \n",
    "\n",
    "Next, we apply **vectorizer.transform** to df_test[\"Review\"], so the saved vocabulary will be used to create test_x. This makes sure train_x and train_y have the same vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee1487c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261, 3323)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_test[\"Review\"])\n",
    "test_y = df_test[\"Sentiment Polarity\"]\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b19ec1",
   "metadata": {},
   "source": [
    "If you are interested, you can see the frequencies of different labels in both sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef537d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>272</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>257</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Train  Test\n",
       "neg    272   122\n",
       "pos    257   139"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Train': train_y.value_counts(),\n",
    "              'Test': test_y.value_counts()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84485db0",
   "metadata": {},
   "source": [
    "In this example, the label equals \"pos\" or \"neg\". **sklearn always sets the alphabetically last value in the class labels as the positive class.** Fortunately, \"pos\" is after \"neg\" in the alphabetical order, so the class labels in this example are intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ff82f",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e8c81",
   "metadata": {},
   "source": [
    "In the backend, SLR is actually built by a **solver**, which is an algorithm that updates the SLR model in iterations to make it better and better. More iterations generally make the model more accurate but also require a longer runtime. \n",
    "\n",
    "The solver stops and returns the SLR model when either one of the following conditions are satisfied:\n",
    "1. **Tolerance**, which is how much SLR changes between iterations, is smaller than a threshold.\n",
    "2. **Max number of iterations** is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5efa78cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1, max_iter=1000, penalty=&#x27;l1&#x27;, random_state=2021,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, max_iter=1000, penalty=&#x27;l1&#x27;, random_state=2021,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1, max_iter=1000, penalty='l1', random_state=2021,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sparselr = LogisticRegression(penalty='l1', \n",
    "                              solver='liblinear',\n",
    "                              random_state=2021,\n",
    "                              tol=0.0001,\n",
    "                              max_iter=1000, \n",
    "                              C=1)\n",
    "sparselr.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb4d83",
   "metadata": {},
   "source": [
    "* **penalty='l1'** means we are using a L1 regularizer, which makes the model a SLR model.  \n",
    "* **solver='liblinear'** selects the 'liblinear' algorithm to build the SLR model. All algorithms are supposed to return the same SLR model, but they needs different amounts of runtime and memory.\n",
    "* **random_state=2021**: Solver uses random search to update model. Fix random_state to make sure the result is reproducible.\n",
    "* **tol=0.0001** sets the threshold for tolerance.\n",
    "* **max_iter=1000** sets the max number of iterations. \n",
    "* **C=1** sets the regularization parameter.\n",
    "\n",
    "For a large data, we usually start with a small max_iter or a large tol, for example, max_iter=100 or tol=0.1, and see how long it takes. If the runtime is bearable, we can increase max_iter or decrease tol, and run the code again.\n",
    "\n",
    "* More solvers: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "\n",
    "* More options: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfeff8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slope betas\n",
    "sparselr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edbc86d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intercept alpha\n",
    "sparselr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c09d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many non-zero betas in total\n",
    "sum(sparselr.coef_[0]!=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda59e10",
   "metadata": {},
   "source": [
    "## Descriptive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba5a44",
   "metadata": {},
   "source": [
    "We can identify the terms that have the largest impacts to pos and neg classes by sorting $\\beta_i$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "395ab5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abromeland/.local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dfbeta = pd.DataFrame({'Term': vectorizer.get_feature_names(),\n",
    "                       'Beta': sparselr.coef_[0]\n",
    "                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3346f81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>1.356127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>perfect</td>\n",
       "      <td>1.064824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sleek</td>\n",
       "      <td>0.980339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amaz</td>\n",
       "      <td>0.746911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bright</td>\n",
       "      <td>0.736774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>easi</td>\n",
       "      <td>0.731250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>favorit</td>\n",
       "      <td>0.520354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>appeal</td>\n",
       "      <td>0.512988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dream</td>\n",
       "      <td>0.476208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>awesom</td>\n",
       "      <td>0.466109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Term      Beta\n",
       "0     love  1.356127\n",
       "1  perfect  1.064824\n",
       "2    sleek  0.980339\n",
       "3     amaz  0.746911\n",
       "4   bright  0.736774\n",
       "5     easi  0.731250\n",
       "6  favorit  0.520354\n",
       "7   appeal  0.512988\n",
       "8    dream  0.476208\n",
       "9   awesom  0.466109"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the most positive terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=False)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0189d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>disappoint</td>\n",
       "      <td>-0.665625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>howev</td>\n",
       "      <td>-0.551987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slow</td>\n",
       "      <td>-0.529074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>expens</td>\n",
       "      <td>-0.468178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>confus</td>\n",
       "      <td>-0.450569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>despit</td>\n",
       "      <td>-0.450490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>seem</td>\n",
       "      <td>-0.449013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>see</td>\n",
       "      <td>-0.420707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alreadi</td>\n",
       "      <td>-0.408001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>poor</td>\n",
       "      <td>-0.407091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Term      Beta\n",
       "0  disappoint -0.665625\n",
       "1       howev -0.551987\n",
       "2        slow -0.529074\n",
       "3      expens -0.468178\n",
       "4      confus -0.450569\n",
       "5      despit -0.450490\n",
       "6        seem -0.449013\n",
       "7         see -0.420707\n",
       "8     alreadi -0.408001\n",
       "9        poor -0.407091"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the most negative terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=True)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588b07f",
   "metadata": {},
   "source": [
    "Please keep in mind that the terms with positive (negative) $\\beta$'s make the documents more likely to be in the positive (negative) class, but the positive class is determined by the alphabetical order. It is just a coincidence that \"pos\" is alphabetically after \"neg\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177dc618",
   "metadata": {},
   "source": [
    "## Predictive Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bed80f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos',\n",
       "       'neg'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply the model to the testing set and predict the class labels\n",
    "sparselr.predict(test_x)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a002b942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.93675779e-01, 6.32422069e-03],\n",
       "       [7.82184934e-01, 2.17815066e-01],\n",
       "       [8.60953266e-01, 1.39046734e-01],\n",
       "       [9.76884247e-01, 2.31157531e-02],\n",
       "       [8.99561288e-01, 1.00438712e-01],\n",
       "       [8.81510285e-01, 1.18489715e-01],\n",
       "       [9.68023742e-01, 3.19762581e-02],\n",
       "       [9.47590638e-01, 5.24093619e-02],\n",
       "       [9.02624031e-09, 9.99999991e-01],\n",
       "       [9.54815700e-01, 4.51842999e-02]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the probability of each doc being in each class \n",
    "#The two columns corresponds to \"neg\" and \"pos\" (in alphabetical order)\n",
    "sparselr.predict_proba(test_x)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8403df",
   "metadata": {},
   "source": [
    "## Performance Metric\n",
    "\n",
    "**Confusion table**:\n",
    "\n",
    "| | Pred Neg | Pred Pos |\n",
    "| --- | --- | --- |\n",
    "| True Neg | 31 | 11 |\n",
    "| True Pos | 48 | 68 |\n",
    "\n",
    "**Accuracy**: The percentage of correct predictions made by a model. \n",
    "\n",
    "- With the confusion table above, accuracy=$\\frac{31+68}{31+68+11+48}=\\%62.7$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e172636",
   "metadata": {},
   "source": [
    "**AUC Score**: \n",
    "\n",
    "- Accuracy metric is problematic for when data is unbalanced (e.g., 990 pos but only 10 neg).\n",
    "- AUC (Area under the ROC Curve) is another performance measure that works better than accuracy on  unbalanced data.\n",
    "\n",
    "\n",
    "\n",
    "- Suppose we rank the data points in the descending order of their predicted probabilities of being positive. AUC measures how well the positive and negative data points are separated after ranking.\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/AUCPredictionsRanked.svg width=\"700\">\n",
    "\n",
    "\n",
    "- AUC represents the probability that a random positive (green) data point is ranked higher than a random negative (red) data point.\n",
    "\n",
    "- AUC ranges in value from 0 to 1 with 1 indicating a perfect model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e952bc3",
   "metadata": {},
   "source": [
    "More performance metrics: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba9808",
   "metadata": {},
   "source": [
    "All of these metrics are callable from **sklearn.metrics**. \n",
    "\n",
    "We often calculate these metrics in both training and testing sets to check if the model is overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d2e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9919c",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c0f415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "0.998109640831758\n",
      "Test:\n",
      "0.8314176245210728\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Test:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4aeedf",
   "metadata": {},
   "source": [
    "AUC Score: The instances will be sorted by the predicted probability of being positive (the alphabetically last class), which is stored in **sparselr.predict_proba(x)[:, 1]**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5284bc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "0.9999928473334859\n",
      "Test:\n",
      "0.915261233636042\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Test:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb15000",
   "metadata": {},
   "source": [
    "This model has a similar performance in training and testing sets, so it is not overfitted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
