{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9481bb6",
   "metadata": {},
   "source": [
    "# Homework 3 | MSCI:6100\n",
    "\n",
    "**10 Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e1069",
   "metadata": {},
   "source": [
    "This assignment has two parts. Each part has questions based on Modules 5, Modules 6 or both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d96fbe0",
   "metadata": {},
   "source": [
    "## Part 1: SMS Spam Detection by Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39190b",
   "metadata": {},
   "source": [
    "The following code reads a collection of SMS messages with each message labeled as **ham** (legitimate) or **spam**. The code also splits **df** into training (70%) and testing (30%) sets as two new data frames called **df_train** and **df_test**. \n",
    "\n",
    "Your task is to use **df_train** to build a predictive model to detect spam messages and test its performance on **df_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985f410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                            Message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read raw data \n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"classdata/spam.csv\")\n",
    "#Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.30, random_state=2021)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a2b9e",
   "metadata": {},
   "source": [
    "### Questions based on only Module 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecf9ea",
   "metadata": {},
   "source": [
    "1a. (0.5 point) To train and test the model, you will first need to construct DTMs and labels. Create DTMs for training and testing sets in any way you like. It is completely your choices to remove stopwords or not, to do stemming or not, to use TF, TFIDF or Binary score, to use n-gram, and to do row normalization or not. Save your DTMs as **train_x** and **test_x**. Create the class labels for training and testing sets. Save them as **train_y** and **test_y**. Print the shapes of the DTMs for training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8256398d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 5974)\n",
      "(1672, 5974)\n"
     ]
    }
   ],
   "source": [
    "#Your answer here:\n",
    "# Needed Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "\n",
    "# Define TFIDF vectorizor with stemming\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\") \n",
    "\n",
    "vectorizer=StemmedTfidfVectorizer(stop_words=nltk_stopwords, norm=None)\n",
    "\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_train[\"Message\"])\n",
    "train_y = df_train[\"Label\"]\n",
    "\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_test[\"Message\"])\n",
    "test_y = df_test[\"Label\"]\n",
    "\n",
    "#Check your answer:\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d014071",
   "metadata": {},
   "source": [
    "1b. (1 point) Create a sparse logistic regression using the DTM and the class lables you created in the previous question. Set the parameters in **LogisticRegression** as follows\n",
    "  - random_state=2021   \n",
    "  - tol=0.001           \n",
    "  - max_iter=1000\n",
    "  - C=0.1\n",
    " \n",
    "Save your model as **sparselr**. Print the number of non-zero betas in  **sparselr**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fb139d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "#Your answer here:\n",
    "# Needed Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "sparselr = LogisticRegression(penalty='l1', \n",
    "                              solver='liblinear',\n",
    "                              random_state=2021,\n",
    "                              tol=0.001,\n",
    "                              max_iter=1000, \n",
    "                              C=0.1)\n",
    "sparselr.fit(train_x,train_y)\n",
    "\n",
    "#Check your solution:\n",
    "print(sum(sparselr.coef_[0]!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1e836",
   "metadata": {},
   "source": [
    "### Questions based on both Modules 5 and 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f87dc4",
   "metadata": {},
   "source": [
    "1c. (1 point) Create a sparse logistic regression using the DTM and the class lables you created in  question 1a. This time you need to select $C$ by 5-fold cross validation from a grid of **20 candidates** that increase proportionally from **l1_min_c** to **l1_min_c$\\times 10^{5}$**.  Since this data is unbalanced, AUC is a better performance metric than accuracy. Use AUC score as the criterion for selecting $C$. Set the parameters in **LogisticRegressionCV** as follows\n",
    "  - random_state=2021   \n",
    "  - tol=0.001           \n",
    "  - max_iter=1000\n",
    "  - scoring='roc_auc' \n",
    " \n",
    "Save your model as **sparselr_cv**. Print the number of non-zero betas in  **sparselr_cv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5042801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "542\n"
     ]
    }
   ],
   "source": [
    "#Your answer here:\n",
    "# Needed Imports\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import l1_min_c\n",
    "import numpy as np\n",
    "\n",
    "param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "sparselr_cv = LogisticRegressionCV(penalty='l1', \n",
    "                                solver='liblinear', \n",
    "                                Cs=param_grid,   \n",
    "                                cv=5,            \n",
    "                                scoring='roc_auc', \n",
    "                                random_state=2021,  \n",
    "                                tol=0.001,\n",
    "                                max_iter=1000)\n",
    "sparselr_cv.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "#Check your solution:\n",
    "print(sum(sparselr_cv.coef_[0]!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114173a",
   "metadata": {},
   "source": [
    "1d. (1 point) Evaluate and print the accuracy and AUC score of **sparselr_cv** from the previous question on the testing and training sets. If you **AUC score on the testing set** is less than 0.985, modify the DTM in your solution for question 1a and re-run your codes for questions 1b and 1c until the AUC score on the testing set in this question is at least 0.985.\n",
    "\n",
    "*Hint: It is your choices to remove stopwords or not, to do stemming or not, to use TF, TFIDF or Binary score, to use n-gram, and to do row normalization or not. There is no best combination that works for all datasets. Just keep trying.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee066d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:\n",
      "1.0\n",
      "Test Accuracy:\n",
      "0.9826555023923444\n",
      "Train AUC:\n",
      "0.9999999999999999\n",
      "Test AUC:\n",
      "0.9873620002074122\n"
     ]
    }
   ],
   "source": [
    "#Your answer here:\n",
    "# imports Needed\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(\"Train Accuracy:\")\n",
    "print(accuracy_score(train_y,sparselr_cv.predict(train_x)))\n",
    "print(\"Test Accuracy:\")\n",
    "print(accuracy_score(test_y,sparselr_cv.predict(test_x)))\n",
    "print(\"Train AUC:\")\n",
    "print(roc_auc_score(train_y,sparselr_cv.predict_proba(train_x)[:, 1]))\n",
    "print(\"Test AUC:\")\n",
    "print(roc_auc_score(test_y,sparselr_cv.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0784b19e",
   "metadata": {},
   "source": [
    "1e.(1 point) Print 10 terms in **sparselr_cv** that have the largest impact to class \"spam\", which means, if these terms appear in a message, that message is more likely to be a spam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc21402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abromeland/.local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146tf150p</td>\n",
       "      <td>1.515027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ac</td>\n",
       "      <td>1.461870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>servic</td>\n",
       "      <td>1.425410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rington</td>\n",
       "      <td>1.321306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voicemail</td>\n",
       "      <td>1.275736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>slower</td>\n",
       "      <td>1.265541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>teenag</td>\n",
       "      <td>1.235977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uk</td>\n",
       "      <td>1.177270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gbp</td>\n",
       "      <td>1.145210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>claim</td>\n",
       "      <td>1.142113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Term      Beta\n",
       "0  146tf150p  1.515027\n",
       "1         ac  1.461870\n",
       "2     servic  1.425410\n",
       "3    rington  1.321306\n",
       "4  voicemail  1.275736\n",
       "5     slower  1.265541\n",
       "6     teenag  1.235977\n",
       "7         uk  1.177270\n",
       "8        gbp  1.145210\n",
       "9      claim  1.142113"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your answer here:\n",
    "\n",
    "dfbeta = pd.DataFrame({'Term': vectorizer.get_feature_names(),\n",
    "                       'Beta': sparselr_cv.coef_[0]\n",
    "                     })\n",
    "\n",
    "#Show the most positive terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=False)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b4996",
   "metadata": {},
   "source": [
    "1f. (1 point) The following code creates a list of three messages. Apply **sparselr_cv** from question 1c to each message and print the predicted class of each message (ham or spam) and the probability of each message being a spam. \n",
    "\n",
    "*Hint: sparselr_cv cannot be directly applied to text. You must first convert the messages to a DTM using the same vectorizer in question 1a. Would you use fit_transform() or transform()?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763c2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NewMessage=[\"\"\"Congrats! 1 year special cinema pass for 2 is yours. \n",
    "                call 09061209465 now! C Suprman V, Matrix3, StarWars3, \n",
    "                etc all 4 FREE! bx420-ip4-5we. 150pm. Dont miss out! \"\"\",\n",
    "            \n",
    "            \"\"\"Update_Now - 12Mths Half Price Orange line rental: \n",
    "               400mins...Call MobileUpd8 on 08000839402 or call2optout=J5Q\"\"\",\n",
    "            \n",
    "            \"\"\"Yo carlos, a few friends are already asking me about you, \n",
    "               you working at all this weekend?\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744c6102",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5974)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['spam', 'spam', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your answer here:\n",
    "#Create the testing DTM and the labels\n",
    "newMessage = vectorizer.transform(NewMessage)\n",
    "\n",
    "#Check your answer:\n",
    "print(newMessage.shape)\n",
    "\n",
    "sparselr_cv.predict(newMessage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10339a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.13467928e-03, 9.96865321e-01],\n",
       "       [5.81316100e-03, 9.94186839e-01],\n",
       "       [9.99997396e-01, 2.60447855e-06]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparselr_cv.predict_proba(newMessage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85aefe8",
   "metadata": {},
   "source": [
    "## Part 2: Predict Stock Price Direction by News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3902f5",
   "metadata": {},
   "source": [
    "The following code loads the data file **\"classdata/BA_newsline_direction.csv\"** into a dataframe called **df2**. This data contains the headlines of the news about The Boeing Company published by Thomson Reuters each day in 2020. See column **headline**. It also contains the moving direction (\"up\" or \"down\") of the stock price of Boeing one day after the news being published. See column **direction**. Note that there can be multiple news on Boeing published on the same day.  If so, each news headline is saved a separated record in **df2**.\n",
    "\n",
    "Your task is to compare the performance of SLR and XGBoost in predicting the price directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aaf7599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>nextday_RET</th>\n",
       "      <th>headline</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>-0.00168</td>\n",
       "      <td>BUZZ-Norwegian Air: Hopes for Boeing deal driv...</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>-0.00168</td>\n",
       "      <td>AIRBUS &lt;AIR.PA&gt; SHARES UP 1.3 PERCENT .</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>-0.00168</td>\n",
       "      <td>AIRBUS &lt;AIR.PA&gt; SHARES EXTEND GAINS, STOCK UP ...</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>-0.00168</td>\n",
       "      <td>HM DUNN AEROSYSTEMS - DOES NOT EXPECT TO FURLO...</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>-0.00168</td>\n",
       "      <td>Reuters Insider - Trading at Noon: Tracking oi...</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  nextday_RET                                           headline  \\\n",
       "0  2020-01-02     -0.00168  BUZZ-Norwegian Air: Hopes for Boeing deal driv...   \n",
       "1  2020-01-02     -0.00168            AIRBUS <AIR.PA> SHARES UP 1.3 PERCENT .   \n",
       "2  2020-01-02     -0.00168  AIRBUS <AIR.PA> SHARES EXTEND GAINS, STOCK UP ...   \n",
       "3  2020-01-02     -0.00168  HM DUNN AEROSYSTEMS - DOES NOT EXPECT TO FURLO...   \n",
       "4  2020-01-02     -0.00168  Reuters Insider - Trading at Noon: Tracking oi...   \n",
       "\n",
       "  direction  \n",
       "0      down  \n",
       "1      down  \n",
       "2      down  \n",
       "3      down  \n",
       "4      down  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_csv(\"classdata/BA_newsline_direction.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d9f11",
   "metadata": {},
   "source": [
    "### Questions based on only Module 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6764ea7",
   "metadata": {},
   "source": [
    "2a. (0.5 point) Split **df2** into training (70%) and testing (30%) sets as two new data frames called **df_train** and **df_test**. Create DTMs for training and testing sets based on the following instructions:\n",
    "\n",
    "- Use the default tokenizer from sklearn library. \n",
    "- Remove stop words in the list of nltk. \n",
    "- Do not stem the terms.\n",
    "- Create DTM in binary scores with using bigrams. \n",
    "\n",
    "Save your DTMs as **train_x** and **test_x**. Create the class labels for training and testing sets. Save them as **train_y** and **test_y**. Print the shapes of the DTMs for training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b48bc164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4090, 23191)\n",
      "(1754, 23191)\n"
     ]
    }
   ],
   "source": [
    "#Your answer here:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df2, test_size=0.30, random_state=2021)\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "vectorizer=CountVectorizer(stop_words=nltk_stopwords, binary=True,ngram_range=(2,2))\n",
    "\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_train[\"headline\"])\n",
    "train_y = df_train[\"direction\"]\n",
    "\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_test[\"headline\"])\n",
    "test_y = df_test[\"direction\"]\n",
    "\n",
    "#Check your answer:\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653aafe0",
   "metadata": {},
   "source": [
    "### Questions based on Module 5 and 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a515d",
   "metadata": {},
   "source": [
    "2b. (1 point) Create a sparse logistic regression using the DTM and the class lables you created in  question 2a. Select $C$ by 5-fold cross validation from a grid of **20 candidates** that increase proportionally from **l1_min_c** to **l1_min_c$\\times 10^{5}$**.  Use **accuracy** as the criterion for selecting $C$. Set the parameters in **LogisticRegressionCV** as follows\n",
    "  - random_state=2021   \n",
    "  - tol=0.001           \n",
    "  - max_iter=1000\n",
    "  - scoring='accuracy' \n",
    " \n",
    "Save your model as **sparselr_cv**. Print the number of non-zero betas in  **sparselr_cv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79093260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17211\n"
     ]
    }
   ],
   "source": [
    "#Your answer here:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import l1_min_c\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "\n",
    "\n",
    "sparselr_cv = LogisticRegressionCV(penalty='l1', \n",
    "                                solver='liblinear', \n",
    "                                Cs=param_grid,   #Use the grid generated above\n",
    "                                cv=5,            #Number of folds, that is, K\n",
    "                                scoring='accuracy', #The performance metric to select the best C.\n",
    "                                random_state=2021,  #To make sure the result is reproducible\n",
    "                                tol=0.001,\n",
    "                                max_iter=1000)\n",
    "sparselr_cv.fit(train_x, train_y)\n",
    "#Check your answer:\n",
    "print(sum(sparselr_cv.coef_[0]!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3e24a",
   "metadata": {},
   "source": [
    "2c. (0.5 point) Evaluate and print the accuracy and AUC score of **sparselr_cv** from the previous question on the testing and training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb0f2386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:\n",
      "0.9946210268948655\n",
      "Test Accuracy:\n",
      "0.7309007981755986\n",
      "Train AUC:\n",
      "0.999938417754819\n",
      "Test AUC:\n",
      "0.8135793865528409\n"
     ]
    }
   ],
   "source": [
    "#Your answer here:\n",
    "print(\"Train Accuracy:\")\n",
    "print(accuracy_score(train_y,sparselr_cv.predict(train_x)))\n",
    "print(\"Test Accuracy:\")\n",
    "print(accuracy_score(test_y,sparselr_cv.predict(test_x)))\n",
    "print(\"Train AUC:\")\n",
    "print(roc_auc_score(train_y,sparselr_cv.predict_proba(train_x)[:, 1]))\n",
    "print(\"Test AUC:\")\n",
    "print(roc_auc_score(test_y,sparselr_cv.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555091d6",
   "metadata": {},
   "source": [
    "2d. (1 point) Use the DTM in question 2a to build a XGBoost model to predict the direction. You need to select parameter 'max_depth' between 2 and 5 and select parameter 'n_estimators' between 10 and 100 by cross validation using **GridSearchCV**. Set the parameters in **XGBClassifier** as follows\n",
    "  - nthread=4\n",
    "  - use_label_encoder=False\n",
    "  - verbosity = 0\n",
    "  - random_state=2021\n",
    "  \n",
    "Set the parameters in **GridSearchCV** as follows\n",
    "  - cv=5\n",
    "  - scoring = 'accuracy'\n",
    "  \n",
    "Save the XGBoost model as **xgb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9db14cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_bin=None,\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None, nthread=4,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=2021, reg_alpha=None, ...),\n",
       "             param_grid={&#x27;max_depth&#x27;: [2, 5], &#x27;n_estimators&#x27;: [10, 100]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_bin=None,\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None, nthread=4,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=2021, reg_alpha=None, ...),\n",
       "             param_grid={&#x27;max_depth&#x27;: [2, 5], &#x27;n_estimators&#x27;: [10, 100]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              nthread=4, num_parallel_tree=None, predictor=None,\n",
       "              random_state=2021, reg_alpha=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, gamma=None,\n",
       "              gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=100, n_jobs=None,\n",
       "              nthread=4, num_parallel_tree=None, predictor=None,\n",
       "              random_state=2021, reg_alpha=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_bin=None,\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None, nthread=4,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=2021, reg_alpha=None, ...),\n",
       "             param_grid={'max_depth': [2, 5], 'n_estimators': [10, 100]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your answer here:\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_y=le.fit_transform(train_y)\n",
    "test_y=le.transform(test_y)\n",
    "\n",
    " \n",
    "param_list = {  \n",
    " 'max_depth':[2, 5],       #Candidate for max_depth\n",
    " 'n_estimators':[10, 100]  #Candidate for n_estimators\n",
    "}\n",
    "xgb=XGBClassifier(nthread=4,\n",
    "                  use_label_encoder=False,\n",
    "                  verbosity = 0,\n",
    "                  random_state=2021\n",
    "                 )\n",
    "xgb = GridSearchCV(estimator = xgb, \n",
    "                   param_grid = param_list,\n",
    "                   scoring = 'accuracy',  #The performance metric to select the best parameters.\n",
    "                   cv=5                   #Number of folds, i.e., K\n",
    "                  )  \n",
    "\n",
    "xgb.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944e828",
   "metadata": {},
   "source": [
    "2e. (0.5 point) Evaluate and print the accuracy and AUC score of **xgb** from the previous question on the testing and training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1d5833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:\n",
      "0.6968215158924206\n",
      "Test Accuracy:\n",
      "0.6174458380843786\n",
      "Train AUC:\n",
      "0.7920542162911943\n",
      "Test AUC:\n",
      "0.6881238387838711\n",
      "{'max_depth': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#Your answer here:\n",
    "\n",
    "\n",
    "print(\"Train Accuracy:\")\n",
    "print(accuracy_score(train_y,xgb.predict(train_x)))\n",
    "print(\"Test Accuracy:\")\n",
    "print(accuracy_score(test_y,xgb.predict(test_x)))\n",
    "print(\"Train AUC:\")\n",
    "print(roc_auc_score(train_y,xgb.predict_proba(train_x)[:, 1]))\n",
    "print(\"Test AUC:\")\n",
    "print(roc_auc_score(test_y,xgb.predict_proba(test_x)[:, 1]))\n",
    "\n",
    "print(xgb.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f608a",
   "metadata": {},
   "source": [
    "2f. (1 point) The following code creates a list of three news headlines. Apply **xgb** to each headline and print the predicted price direction (encoded as 1 or 0) after the news and the probability of moving in each direction.\n",
    "\n",
    "*Hint: xgb cannot be directly applied to text. You must first convert the messages to a DTM using the same vectorizer in question 1a.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7210e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "News=[\"\"\" BRIEF-United Airlines Is Set To Take Delivery Of \n",
    "          A 737 Max From Boeing As Early As Tuesday - CNBC \"\"\",\n",
    "            \n",
    "     \"\"\" BOEING - DELIVERIES TO THE RNLAF ARE EXPECTED TO CONTINUE INTO 2021.\"\"\",\n",
    "            \n",
    "     \"\"\"BRIEF-United Airlines Holdings Says Entered Agreement With Unit Of BOC \n",
    "           Aviation Ltd To Finance Through Sale,Lease Transaction 6 Boeing 787-9 Aircraft\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b24f8b46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your answer here:\n",
    "\n",
    "xgb=XGBClassifier(max_depth=5,\n",
    "                  n_estimators=100,\n",
    "                  nthread=4,\n",
    "                  use_label_encoder=False,\n",
    "                  verbosity = 0,\n",
    "                  random_state=2021\n",
    "                 )\n",
    "xgb.fit(train_x, train_y)\n",
    "\n",
    "news = vectorizer.transform(News)\n",
    "\n",
    "xgb.predict(news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79cda8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47590506, 0.52409494],\n",
       "       [0.51115847, 0.48884156],\n",
       "       [0.7261682 , 0.27383178]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.predict_proba(news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
