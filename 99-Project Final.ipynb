{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project and Presentation - Text Analytics\n",
    "## Aaron Bromeland\n",
    "\n",
    "### Purpose:\n",
    "- To obtain data from the SEC edgar search and stock market to see the impact of reportings to the SEC on stock valutaion of Deere & Co. Will look to develop a model with Deere and then try to apply it to other companies within the industry and then potentially other companies outside of the Agricultural/Construction and Forestry Industry. \n",
    "\n",
    "- Background:\n",
    "     - Deere & Company is an Agriculture and Construction and Forestry manufacturer that is headquartered in Moline, Illinois. Deere & Company started in 1837 after John Deere invented the self scouring plow in 1836 that allowed Midwest farmers to plow the thick black dirt of the Midwest without clogging. The company started in the town of Grad Detour, Illinois, and was moved in 1848 to Moline, Illinois. The company started with the technological advancement of the plow that was made from steel instead of cast iron and has since led the industry in technological advances in agriculture all the way to today with the introduction of the first commercially available fully autonomous tractor.  The company was formally incorporated as Deere & Company in 1868. John Deere served as the Founder and president of the company from 1837-1886. Then decedents of Deere served as President and Chairman from 1886-1982 (Charles Deere, William Butterworth, Charles Deere Wiman, William Hewitt). Then from 1982-today, John Deere has been run by members outside of the Deere family (Robert Hanson, Hans Becherer, Robert Lane, Samuel Allen, and John May)\n",
    "\n",
    "- Project Goals\n",
    "    - The project is hoping to identify trends in SEC filings that correlate to positive or negative stock returns. Specifically, we are looking to identify long term trends in the data to determine if SEC filings can help point to long-term growth opportunities in the stock of Deere. Additionally, we would like to extend this analysis beyond Deere to other companies first in the Agricultural and Construction and Forestry industries, and if we get successful results, then extend this to other company filings, as well. \n",
    "\n",
    "\n",
    "- Research Questions\n",
    "    1.\tWhich standard forms filed with the SEC have the highest sentiment scores?\n",
    "    2.\tWhich standard forms filed with the SEC have the lowest sentiment scores? \n",
    "    3.\tWhich standard forms filed with the SEC have the highest variance in sentiment scores?\n",
    "    4.\tWhich filings coincide with the greatest change in stock price for the company? \n",
    "    5.\tDo SEC filings have a material impact on the stock price of Deere & Co?\n",
    "    6.\tCan the sentiment analysis developed for Deere & Co be applied to other companies within the agricultural and construction industry?\n",
    "    7.\tCan the sentiment analysis developed for Deere & Co be applied to other companies outside of Deereâ€™s industry? \n",
    "\n",
    "\n",
    "- Data Source\n",
    "    - The dataset is comprised of 1000 filings of Deere with the SEC from October 3rd, 2011 to May, 31st, 2022. Initially, the shortest document in the dataset was 37 characters long with average character lengths of 24,110. However, 38 documents had under 500 characters in them. These were removed because after review of these documents, they were for PDF files that had been loaded or were references to other paper submissions that had been done.  After removing these documents, the average character lengths of the documents were 25,054 with a maximum character length of 564,539 and minimum character length of 539.\n",
    "\n",
    "\n",
    "### Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from lxml import html\n",
    "#!pip3 install yfinance\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "#!pip3 install afinn\n",
    "from afinn import Afinn\n",
    "#!pip3 install textblob\n",
    "from textblob import TextBlob\n",
    "#!pip3 install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#!pip3 install pysentiment2\n",
    "import pysentiment2 as ps\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt                      # a library for visualization\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.svm import l1_min_c\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation  #LDA module from sklearn. \n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#!pip install scipy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull SEC Data\n",
    "- Pull data via the SEC API's that are made available. This pulls by company CIK number. Using the CIK number we can get all the recent filings for a company. Then we used the information from the first data pull to pull all of the text content for the items that have been previously submitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request Data from Edgar Search. Pulled by Company CIK number\n",
    "data = requests.get('https://data.sec.gov/submissions/CIK0000315189.json',headers={'User-Agent':'University of Iowa abromeland@uiowa.edu'})\n",
    "data = json.loads(data.content)['filings']['recent']\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.sort_values(by='filingDate',ascending=False,inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data by accessionNumber and FileName to be added to the dataframe\n",
    "for i in range(len(df)):\n",
    "    accessionNumber = df.loc[i,\"accessionNumber\"]\n",
    "    filename = df.loc[i,'primaryDocument']\n",
    "    URL = f'https://www.sec.gov/Archives/edgar/data/315189/{re.sub(\"-\",\"\",accessionNumber)}/{filename}'\n",
    "    page = requests.get(URL,headers={'User-Agent':'University of Iowa abromeland@uiowa.edu'})\n",
    "    root = html.fromstring(page.content)\n",
    "    text = [s.text_content() for s in root.xpath('/html/body')]\n",
    "    df.loc[i,'text'] = ' '.join(text)\n",
    "    time.sleep(.5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data and print out example data\n",
    "df['text'] = df['text'].str.replace(\"[\\\\u200b||\\\\n||\\\\xa0]\",'',regex=True)\n",
    "df['text'] = df['text'].str.replace(\"\\$\\d+[\\d,\\.]*\",'moneyToken',regex=True)\n",
    "df['text'] = df['text'].str.lower()\n",
    "print(df.text[0])\n",
    "print(df.text[1])\n",
    "print(df.text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Filing Date nulls, and save filing date so it can be used later in graphing and to merge datasets.\n",
    "print(df.filingDate.isnull().sum())\n",
    "print(df.filingDate.dtype)\n",
    "df[\"filingDate\"] = pd.to_datetime(df[\"filingDate\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "print(df.filingDate.dtype)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape and save the data for later use\n",
    "print(df.shape)\n",
    "df.to_csv(\"Deere_SEC.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Stock Market Data\n",
    "- Obtained from the yfinance API. \n",
    "- Will be used to generate if the stock is decreasing or increasing in a given time period.\n",
    "- Added the columns \n",
    "    1. dayDiff - Holds the differences that are recorded daily\n",
    "    2. weekDiff - Holds the differences that are recorded for 5 business days (week)\n",
    "    3. monthDiff - Holds the differences that are recorded monthly. 5x4 + 2 - Five Days over four weeks, and add two days for the quarters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price = yf.download(\"DE\",start='2011-08-01',end='2022-07-20',progress=False)\n",
    "df_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_price.index))\n",
    "df_price.sort_index(ascending=False,inplace=True)\n",
    "df_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price['dayDiff'] = -1*df_price['Adj Close'].diff()\n",
    "df_price['weekDiff'] = -1*df_price['Adj Close'].diff(periods=5)\n",
    "df_price['monthDiff'] = -1*df_price['Adj Close'].diff(periods=22)\n",
    "df_price.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis \n",
    "\n",
    "- Running the sentiment Analysis of all documents, and will plot this against the changes in stock price that are obtained for the same time period. \n",
    "- The sentiment analizers that were used are below:\n",
    "    1. AFINN\n",
    "    2. TextBlob\n",
    "    3. VADER\n",
    "    4. LM - Loughran and McDonald Financial Sentiment Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING - THIS CELL WILL TAKE A LONG TIME TO RUN\n",
    "\n",
    "# AFINN Sentiment Analysis Scores\n",
    "afinn = Afinn(emoticons=True)\n",
    "df[\"AFINN\"]=[afinn.score(s) for s in df.text]\n",
    "\n",
    "# TextBlob Sentiment Analysis Scores\n",
    "df[\"TextBlob\"]=[TextBlob(s).sentiment.polarity for s in df.text]\n",
    "\n",
    "# VADER Sentiment Analysis Scores\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "df[\"VADER\"]=[analyzer.polarity_scores(s)['compound'] for s in df.text]\n",
    "\n",
    "# Loughran and McDonald Sentiment Scores\n",
    "lm = ps.LM()\n",
    "df['LMTitle'] = 0\n",
    "for i in range(len(df['text'])):\n",
    "    tokens = lm.tokenize(df['text'][i])\n",
    "    score = lm.get_score(tokens)\n",
    "    df.loc[i,\"LMTitle\"]=score[\"Polarity\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Out Results\n",
    "df.to_csv(\"Deere_Sentiment_Scores.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sentiment Analysis \n",
    "    1. Graph All Sentiment Scores together\n",
    "    2. Graph Daily, Weekly, and Monthly Changes\n",
    "    3. Merge Sentiment Scores with Stock Data \n",
    "    4. Graph Stock data with Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily, weekly, and monthly difference\n",
    "\n",
    "plt.plot(df_price.index, df_price['dayDiff'].rolling(window=50).mean(), \"-g\", label=\"Daily Difference\")\n",
    "plt.plot(df_price.index, df_price['weekDiff'].rolling(window=50).mean(), \"-b\", label=\"Weekly Difference\")\n",
    "plt.plot(df_price.index, df_price['monthDiff'].rolling(window=50).mean(), \"-r\", label=\"Monthly Difference\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Ticker Price - Adjusted Close Difference\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Stock Price (Moving Average with Window Size 50)\")\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-load Location - Allows for csv to be reloaded and merged with original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-load data set location - uncomment to re-laod \n",
    "#df = pd.read_csv(\"Deere_Sentiment_Scores.csv\")\n",
    "#print(df.filingDate.isnull().sum())\n",
    "#print(df.filingDate.dtype)\n",
    "#df[\"filingDate\"] = pd.to_datetime(df[\"filingDate\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "#print(df.filingDate.dtype)\n",
    "#df['AFINN_SCALE'] = MinMaxScaler().fit_transform(np.array(df['AFINN']).reshape(-1,1))\n",
    "\n",
    "# Merge Stock and Sentiment Data\n",
    "df_all = pd.merge(df,df_price,how='left',left_on = 'filingDate',right_index = True, copy=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out Data set so it can be used and re-loaded at a later date.\n",
    "df_all.to_csv(\"Deere_All.csv\",index=False)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph all stock data with Sentiment Data - First Scale difference data to be graphed and compared with sentiment data.\n",
    "df_all['dayDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_all['dayDiff']).reshape(-1,1))\n",
    "df_all['weekDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_all['weekDiff']).reshape(-1,1))\n",
    "df_all['monthDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_all['monthDiff']).reshape(-1,1))\n",
    "df_all['Adj_Close_SCALE'] = MinMaxScaler().fit_transform(np.array(df_all['Adj Close']).reshape(-1,1))\n",
    "\n",
    "\n",
    "# inline display of plots\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(df_all.filingDate, df_all.AFINN_SCALE.rolling(window=50).mean(), \"-b\", label=\"AFINN Scaled\")\n",
    "plt.plot(df_all.filingDate, df_all.TextBlob.rolling(window=50).mean(), \"-g\", label=\"TextBlob\")\n",
    "plt.plot(df_all.filingDate, df_all.VADER.rolling(window=50).mean(), \"-r\", label=\"VADER\")\n",
    "plt.plot(df_all.filingDate, df_all.LMTitle.rolling(window=50).mean(), \"-c\", label=\"LM\")\n",
    "plt.plot(df_all.filingDate, df_all.dayDiff_SCALE.rolling(window=50).mean(), \"-m\", label=\"Day Difference\")\n",
    "plt.plot(df_all.filingDate, df_all.weekDiff_SCALE.rolling(window=50).mean(), \"-y\", label=\"Week Difference\")\n",
    "plt.plot(df_all.filingDate, df_all.monthDiff_SCALE.rolling(window=50).mean(), \"-k\", label=\"Month Difference\")\n",
    "plt.plot(df_all.filingDate, df_all.Adj_Close_SCALE.rolling(window=50).mean(), \"-p\", label=\"Adjusted Close\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Comparison on Sentiment Score\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment Score (Moving Average with Window Size 50)\")\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Sentiment scores with Adjusted Closing Values\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(df_all.filingDate, df_all.AFINN_SCALE.rolling(window=50).mean(), \"-b\", label=\"AFINN Scaled\")\n",
    "plt.plot(df_all.filingDate, df_all.TextBlob.rolling(window=50).mean(), \"-g\", label=\"TextBlob\")\n",
    "plt.plot(df_all.filingDate, df_all.VADER.rolling(window=50).mean(), \"-r\", label=\"VADER\")\n",
    "plt.plot(df_all.filingDate, df_all.LMTitle.rolling(window=50).mean(), \"-k\", label=\"LM\")\n",
    "plt.plot(df_all.filingDate, df_all.Adj_Close_SCALE.rolling(window=50).mean(), \"-c\", label=\"Adjusted Close\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Comparison on Sentiment Score\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment Score (Moving Average with Window Size 50)\")\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics on Data Set\n",
    "\n",
    "1. Obtain the character counts and throw out data that is too small.\n",
    "2. Create DTM's\n",
    "    1. Unigrams\n",
    "    2. Bigrams\n",
    "    3. Trigrams\n",
    "    \n",
    "    \n",
    "#### Remove and Review of Small Character Count Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['characters'] = [len(s) for s in df_all['text']]\n",
    "print(f\"Average character length: {df_all['characters'].mean()}\")\n",
    "print(f\"Minimum character length: {df_all['characters'].min()}\")\n",
    "print(f\"Maximum character length: {df_all['characters'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_all[df_all['characters']<1000].copy()\n",
    "print(df_small.shape)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_small['text'][556]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_small['text'][991]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[df_all['characters']>500].copy()\n",
    "df_all.sort_values(by='filingDate',ascending=False,inplace=True)\n",
    "df_all.reset_index(inplace=True,drop=True)\n",
    "print(df_all.shape)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average character length: {df_all['characters'].mean()}\")\n",
    "print(f\"Minimum character length: {df_all['characters'].min()}\")\n",
    "print(f\"Maximum character length: {df_all['characters'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation Of DTM's\n",
    "\n",
    "##### Unigrams - Use of LM Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['moneytoken','font','serif','famili','helvetica','size',\n",
    "                                                            'includ','arial','px','form','weight','color','text',\n",
    "                                                            'align','de','file','solid','e','c','b','k','f','r',\n",
    "                    'p','g','n','q','h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LM dictionary for DTM - No Stop word Removal\n",
    "lm = ps.LM()\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize)\n",
    "DTM = vectorizer.fit_transform(df_all['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffreq = pd.DataFrame({'Term': vectorizer.get_feature_names_out(),\n",
    "                   'Frequency': DTM.sum(axis=0).tolist()[0]\n",
    "                  })\n",
    "\n",
    "dffreq.sort_values(by=\"Frequency\",inplace=True,ascending=False)\n",
    "dffreq.reset_index(inplace=True,drop=True)\n",
    "dffreq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LM dictionary for DTM - No Stop word Removal except custom stop words\n",
    "lm = ps.LM()\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,stop_words=custom_stop_words)\n",
    "DTM = vectorizer.fit_transform(df_all['text'])\n",
    "dffreq = pd.DataFrame({'Term': vectorizer.get_feature_names_out(),\n",
    "                   'Frequency': DTM.sum(axis=0).tolist()[0]\n",
    "                  })\n",
    "\n",
    "dffreq.sort_values(by=\"Frequency\",inplace=True,ascending=False)\n",
    "dffreq.reset_index(inplace=True,drop=True)\n",
    "dffreq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LM dictionary for DTM - Stop Word Removal\n",
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\") + custom_stop_words\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,stop_words=nltk_stopwords)\n",
    "DTM = vectorizer.fit_transform(df_all['text'])\n",
    "dffreq = pd.DataFrame({'Term': vectorizer.get_feature_names_out(),\n",
    "                   'Frequency': DTM.sum(axis=0).tolist()[0]\n",
    "                  })\n",
    "\n",
    "dffreq.sort_values(by=\"Frequency\",inplace=True,ascending=False)\n",
    "dffreq.reset_index(inplace=True,drop=True)\n",
    "dffreq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bi-grams - Use LM Dictionary and custom Stop word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,ngram_range=(2,2),stop_words=custom_stop_words)\n",
    "DTM = vectorizer.fit_transform(df_all['text'])\n",
    "dffreq = pd.DataFrame({'Term': vectorizer.get_feature_names_out(),\n",
    "                   'Frequency': DTM.sum(axis=0).tolist()[0]\n",
    "                  })\n",
    "\n",
    "dffreq.sort_values(by=\"Frequency\",inplace=True,ascending=False)\n",
    "dffreq.reset_index(inplace=True,drop=True)\n",
    "dffreq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\") + custom_stop_words\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,stop_words=nltk_stopwords,ngram_range=(2,2))\n",
    "DTM = vectorizer.fit_transform(df_all['text'])\n",
    "dffreq = pd.DataFrame({'Term': vectorizer.get_feature_names_out(),\n",
    "                   'Frequency': DTM.sum(axis=0).tolist()[0]\n",
    "                  })\n",
    "\n",
    "dffreq.sort_values(by=\"Frequency\",inplace=True,ascending=False)\n",
    "dffreq.reset_index(inplace=True,drop=True)\n",
    "dffreq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tri-grams - LM Tokenizer with custom stop word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,ngram_range=(3,3),stop_words = custom_stop_words)\n",
    "DTM = vectorizer.fit_transform(df_all['text'])\n",
    "dffreq = pd.DataFrame({'Term': vectorizer.get_feature_names_out(),\n",
    "                   'Frequency': DTM.sum(axis=0).tolist()[0]\n",
    "                  })\n",
    "\n",
    "dffreq.sort_values(by=\"Frequency\",inplace=True,ascending=False)\n",
    "dffreq.reset_index(inplace=True,drop=True)\n",
    "dffreq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\") + custom_stop_words\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,stop_words=nltk_stopwords,ngram_range=(3,3))\n",
    "DTM = vectorizer.fit_transform(df_all['text'])\n",
    "dffreq = pd.DataFrame({'Term': vectorizer.get_feature_names_out(),\n",
    "                   'Frequency': DTM.sum(axis=0).tolist()[0]\n",
    "                  })\n",
    "\n",
    "dffreq.sort_values(by=\"Frequency\",inplace=True,ascending=False)\n",
    "dffreq.reset_index(inplace=True,drop=True)\n",
    "dffreq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\") + custom_stop_words\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,stop_words=nltk_stopwords,ngram_range=(1,1))\n",
    "DTM =vectorizer.fit_transform(df_all['text'])\n",
    "\n",
    "\n",
    "num_topics=[1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "lda = LatentDirichletAllocation(n_jobs=-1,   \n",
    "                                max_iter=10,  \n",
    "                                random_state=2021 \n",
    "                               )\n",
    "perplexity=[]\n",
    "for i in num_topics:\n",
    "    print(i)\n",
    "    lda.set_params(n_components=i)\n",
    "    lda.fit(DTM)\n",
    "    perplexity.append(lda.perplexity(DTM))\n",
    "\n",
    "plt.plot(num_topics, perplexity)\n",
    "plt.xlabel('Num. of Topics')\n",
    "plt.ylabel('Perplexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=4,\n",
    "                                n_jobs=-1,   \n",
    "                                max_iter=20,   \n",
    "                                random_state=2021 \n",
    "                               )\n",
    "lda.fit(DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the top words for each topic and put them together in the same data frame.\n",
    "temparray = preprocessing.normalize(lda.components_,norm=\"l1\")\n",
    "TTopicM = pd.DataFrame(np.transpose(temparray), index = vectorizer.get_feature_names())\n",
    "TermOfTopic =pd.DataFrame([])\n",
    "for i in range(4):\n",
    "    TermOfTopic[i]=(list(TTopicM.sort_values(by=i,ascending=False).iloc[:10,i].index))\n",
    "TermOfTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_all[['form','text','filingDate']]\n",
    "DTopicM = pd.DataFrame(lda.transform(DTM))\n",
    "dfnew = pd.concat([df_main, DTopicM], axis=1)\n",
    "dfnew.sort_values(by=0,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.sort_values(by=1,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.text[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.sort_values(by=2,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.sort_values(by=3,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\") + custom_stop_words\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,stop_words=nltk_stopwords,ngram_range=(2,2))\n",
    "DTM =vectorizer.fit_transform(df_all['text'])\n",
    "\n",
    "\n",
    "num_topics=[1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "lda = LatentDirichletAllocation(n_jobs=-1,   \n",
    "                                max_iter=10,  \n",
    "                                random_state=2021 \n",
    "                               )\n",
    "perplexity=[]\n",
    "for i in num_topics:\n",
    "    print(i)\n",
    "    lda.set_params(n_components=i)\n",
    "    lda.fit(DTM)\n",
    "    perplexity.append(lda.perplexity(DTM))\n",
    "\n",
    "plt.plot(num_topics, perplexity)\n",
    "plt.xlabel('Num. of Topics')\n",
    "plt.ylabel('Perplexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3,\n",
    "                                n_jobs=-1,   \n",
    "                                max_iter=20,   \n",
    "                                random_state=2021 \n",
    "                               )\n",
    "lda.fit(DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the top words for each topic and put them together in the same data frame.\n",
    "temparray = preprocessing.normalize(lda.components_,norm=\"l1\")\n",
    "TTopicM = pd.DataFrame(np.transpose(temparray), index = vectorizer.get_feature_names())\n",
    "TermOfTopic =pd.DataFrame([])\n",
    "for i in range(3):\n",
    "    TermOfTopic[i]=(list(TTopicM.sort_values(by=i,ascending=False).iloc[:10,i].index))\n",
    "TermOfTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_all[['form','text','filingDate']]\n",
    "DTopicM = pd.DataFrame(lda.transform(DTM))\n",
    "dfnew = pd.concat([df_main, DTopicM], axis=1)\n",
    "dfnew.sort_values(by=0,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.sort_values(by=1,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.sort_values(by=2,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words(\"english\") + custom_stop_words\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,stop_words=nltk_stopwords,ngram_range=(3,3))\n",
    "DTM =vectorizer.fit_transform(df_all['text'])\n",
    "\n",
    "\n",
    "num_topics=[1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "lda = LatentDirichletAllocation(n_jobs=-1,   \n",
    "                                max_iter=10,  \n",
    "                                random_state=2021 \n",
    "                               )\n",
    "perplexity=[]\n",
    "for i in num_topics:\n",
    "    print(i)\n",
    "    lda.set_params(n_components=i)\n",
    "    lda.fit(DTM)\n",
    "    perplexity.append(lda.perplexity(DTM))\n",
    "\n",
    "plt.plot(num_topics, perplexity)\n",
    "plt.xlabel('Num. of Topics')\n",
    "plt.ylabel('Perplexity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3,\n",
    "                                n_jobs=-1,   \n",
    "                                max_iter=20,   \n",
    "                                random_state=2021 \n",
    "                               )\n",
    "lda.fit(DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the top words for each topic and put them together in the same data frame.\n",
    "temparray = preprocessing.normalize(lda.components_,norm=\"l1\")\n",
    "TTopicM = pd.DataFrame(np.transpose(temparray), index = vectorizer.get_feature_names())\n",
    "TermOfTopic =pd.DataFrame([])\n",
    "for i in range(3):\n",
    "    TermOfTopic[i]=(list(TTopicM.sort_values(by=i,ascending=False).iloc[:10,i].index))\n",
    "TermOfTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_all[['form','text','filingDate']]\n",
    "DTopicM = pd.DataFrame(lda.transform(DTM))\n",
    "dfnew = pd.concat([df_main, DTopicM], axis=1)\n",
    "dfnew.sort_values(by=0,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.sort_values(by=1,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.sort_values(by=2,ascending=False,inplace=True)\n",
    "dfnew.reset_index(inplace=True,drop=True)\n",
    "dfnew.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Questions\n",
    "\n",
    "1.\tWhich standard forms filed with the SEC have the highest sentiment scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forms = df.copy()\n",
    "df_forms.sort_values(by='form',ascending=False,inplace=True)\n",
    "df_forms.reset_index(inplace=True,drop=True)\n",
    "df_forms = df_forms.set_index(['form',df.index])\n",
    "df_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df_forms[['AFINN','TextBlob','VADER','LMTitle']].groupby(level='form').mean()\n",
    "df_sent = pd.DataFrame([])\n",
    "for i in ['AFINN','TextBlob','VADER','LMTitle']:\n",
    "    df_sent[i] = (list(sentiment.sort_values(by=i,ascending=False)[i][:10].index))\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df_forms[['AFINN','TextBlob','VADER','LMTitle']].groupby(level='form').max()\n",
    "df_sent = pd.DataFrame([])\n",
    "for i in ['AFINN','TextBlob','VADER','LMTitle']:\n",
    "    df_sent[i] = (list(sentiment.sort_values(by=i,ascending=False)[i][:10].index))\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df_forms[['AFINN','TextBlob','VADER','LMTitle']].groupby(level='form').min()\n",
    "df_sent = pd.DataFrame([])\n",
    "for i in ['AFINN','TextBlob','VADER','LMTitle']:\n",
    "    df_sent[i] = (list(sentiment.sort_values(by=i,ascending=True)[i][:10].index))\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df_forms[['AFINN','TextBlob','VADER','LMTitle']].groupby(level='form').var()\n",
    "df_sent = pd.DataFrame([])\n",
    "for i in ['AFINN','TextBlob','VADER','LMTitle']:\n",
    "    df_sent[i] = (list(sentiment.sort_values(by=i,ascending=False)[i][:10].index))\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df_forms[['AFINN','TextBlob','VADER','LMTitle']].groupby(level='form').std()\n",
    "df_sent = pd.DataFrame([])\n",
    "for i in ['AFINN','TextBlob','VADER','LMTitle']:\n",
    "    df_sent[i] = (list(sentiment.sort_values(by=i,ascending=False)[i][:10].index))\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_all[df_all['LMTitle'] == df_all['LMTitle'].max()])\n",
    "#df_all['text'][842]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8K = df_all[df_all['form']=='8-K']\n",
    "max_form = df_8K[df_8K['LMTitle'] == df_8K['LMTitle'].max()]\n",
    "max_form = max_form[['form','filingDate','LMTitle','Adj Close']]\n",
    "max_form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_all[df_all['LMTitle'] == df_all['LMTitle'].min()])\n",
    "#print(df_all['text'][139])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forms = df_all.copy()\n",
    "df_forms.sort_values(by='form',ascending=False,inplace=True)\n",
    "df_forms.reset_index(inplace=True,drop=True)\n",
    "df_forms = df_forms.set_index(['form',df_all.index])\n",
    "df_forms\n",
    "print(df_forms[['dayDiff','weekDiff','monthDiff']].groupby(level='form').max())\n",
    "priceDiff = df_forms[['dayDiff','weekDiff','monthDiff']].groupby(level='form').max().abs()\n",
    "print(priceDiff)\n",
    "df_sent = pd.DataFrame([])\n",
    "for i in ['dayDiff','weekDiff','monthDiff']:\n",
    "    df_sent[i] = (list(priceDiff.sort_values(by=i,ascending=False)[i][:10].index))\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all[df_all['dayDiff']==df_all['dayDiff'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all[df_all['weekDiff']==df_all['weekDiff'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8K = df_all[df_all['form'] == '8-K']\n",
    "print(df_8K[df_8K['weekDiff']==df_8K['weekDiff'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all[df_all['monthDiff']==df_all['monthDiff'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_forms[['dayDiff','weekDiff','monthDiff']].groupby(level='form').mean())\n",
    "priceDiff = df_forms[['dayDiff','weekDiff','monthDiff']].groupby(level='form').mean().abs()\n",
    "print(priceDiff)\n",
    "df_sent = pd.DataFrame([])\n",
    "for i in ['dayDiff','weekDiff','monthDiff']:\n",
    "    df_sent[i] = (list(priceDiff.sort_values(by=i,ascending=False)[i][:10].index))\n",
    "df_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Logistic Regression\n",
    "\n",
    "1. Dataset Preparation\n",
    "    - Add column for daily difference - positive or negative\n",
    "    - Add column for weekly difference - positive or negative\n",
    "    - Add column for monthly difference - positive or negative\n",
    "    - Data was already cleaned in prior steps\n",
    "    - Create DTMs for training and testing data\n",
    "2. Feature Engineering\n",
    "3. Model Training\n",
    "4. Descriptive Analytics\n",
    "5. Performance Metric\n",
    "\n",
    "### Daily Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[[\"text\",\"dayDiff\"]].copy()\n",
    "df['dayDiff'] = np.where(df.dayDiff > 0 ,'postive','negative')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.33, random_state=2021)\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ps.LM()\n",
    "vectorizer = TfidfVectorizer(tokenizer = lm.tokenize,stop_words=custom_stop_words)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_train[\"text\"])\n",
    "train_y = df_train[\"dayDiff\"]\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_test[\"text\"])\n",
    "test_y = df_test[\"dayDiff\"]\n",
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Train': train_y.value_counts(),\n",
    "              'Test': test_y.value_counts()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparselr = LogisticRegression(penalty='l1', \n",
    "                              solver='liblinear',\n",
    "                              random_state=2021,\n",
    "                              tol=0.0001,\n",
    "                              max_iter=1000, \n",
    "                              C=1)\n",
    "sparselr.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many non-zero betas in total\n",
    "sum(sparselr.coef_[0]!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbeta = pd.DataFrame({'Term': vectorizer.get_feature_names(),\n",
    "                       'Beta': sparselr.coef_[0]\n",
    "                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the most positive terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=False)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the most negative terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=True)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Train:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Test:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Score\n",
    "print(\"Train:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Test:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[[\"text\",\"weekDiff\"]].copy()\n",
    "df['weekDiff'] = np.where(df.weekDiff > 0 ,'postive','negative')\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.33, random_state=2021)\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "lm = ps.LM()\n",
    "vectorizer = TfidfVectorizer(tokenizer = lm.tokenize,stop_words=custom_stop_words)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_train[\"text\"])\n",
    "train_y = df_train[\"weekDiff\"]\n",
    "train_x.shape\n",
    "\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_test[\"text\"])\n",
    "test_y = df_test[\"weekDiff\"]\n",
    "test_x.shape\n",
    "\n",
    "print(pd.DataFrame({'Train': train_y.value_counts(),\n",
    "              'Test': test_y.value_counts()}))\n",
    "\n",
    "sparselr = LogisticRegression(penalty='l1', \n",
    "                              solver='liblinear',\n",
    "                              random_state=2021,\n",
    "                              tol=0.0001,\n",
    "                              max_iter=1000, \n",
    "                              C=1)\n",
    "sparselr.fit(train_x,train_y)\n",
    "\n",
    "#How many non-zero betas in total\n",
    "print(sum(sparselr.coef_[0]!=0))\n",
    "\n",
    "dfbeta = pd.DataFrame({'Term': vectorizer.get_feature_names(),\n",
    "                       'Beta': sparselr.coef_[0]\n",
    "                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the most positive terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=False)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the most negative terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=True)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Train:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Test:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Score\n",
    "print(\"Train:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Test:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[[\"text\",\"monthDiff\"]].copy()\n",
    "df['monthDiff'] = np.where(df.monthDiff > 0 ,'postive','negative')\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.33, random_state=2021)\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "lm = ps.LM()\n",
    "vectorizer = TfidfVectorizer(tokenizer = lm.tokenize,stop_words=custom_stop_words)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_train[\"text\"])\n",
    "train_y = df_train[\"monthDiff\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_test[\"text\"])\n",
    "test_y = df_test[\"monthDiff\"]\n",
    "print(test_x.shape)\n",
    "\n",
    "print(pd.DataFrame({'Train': train_y.value_counts(),\n",
    "              'Test': test_y.value_counts()}))\n",
    "\n",
    "sparselr = LogisticRegression(penalty='l1', \n",
    "                              solver='liblinear',\n",
    "                              random_state=2021,\n",
    "                              tol=0.0001,\n",
    "                              max_iter=1000, \n",
    "                              C=1)\n",
    "sparselr.fit(train_x,train_y)\n",
    "\n",
    "#How many non-zero betas in total\n",
    "print(sum(sparselr.coef_[0]!=0))\n",
    "\n",
    "dfbeta = pd.DataFrame({'Term': vectorizer.get_feature_names(),\n",
    "                       'Beta': sparselr.coef_[0]\n",
    "                     })\n",
    "\n",
    "#Show the most positive terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=False)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the most negative terms\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=True)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "print(\"Train:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Test:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "\n",
    "# AUC Score\n",
    "print(\"Train:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Test:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Analysis\n",
    "    - First Attempt at Regression Analysis using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[[\"text\",\"Adj Close\"]].copy()\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.33, random_state=2021)\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "lm = ps.LM()\n",
    "vectorizer = TfidfVectorizer(tokenizer = lm.tokenize,stop_words=custom_stop_words)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_train[\"text\"])\n",
    "train_y = df_train[\"Adj Close\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_test[\"text\"])\n",
    "test_y = df_test[\"Adj Close\"]\n",
    "print(test_x.shape)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(train_x)\n",
    "train_x=scaler.transform(train_x)\n",
    "test_x=scaler.transform(test_x)\n",
    "\n",
    "lasso = Lasso(alpha=0.01, #Regularization parameter.\n",
    "              max_iter=5000\n",
    "              )\n",
    "lasso.fit(train_x,train_y)\n",
    "print(np.count_nonzero(lasso.coef_))\n",
    "#Check the percentage of non-zero slopes\n",
    "print(np.count_nonzero(lasso.coef_)/len(lasso.coef_))\n",
    "\n",
    "dfbeta = pd.DataFrame({'Term': vectorizer.get_feature_names(),\n",
    "                       'Beta': lasso.coef_\n",
    "                     })\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=False)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=True)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the model to the reviews testing set and predict the star ratings\n",
    "print(lasso.predict(test_x)[0:10])\n",
    "print(test_y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error:\")\n",
    "print(\"Training:\")\n",
    "print(mean_squared_error(train_y,lasso.predict(train_x),squared=False))\n",
    "print()\n",
    "print(\"Testing:\")\n",
    "print(mean_squared_error(test_y,lasso.predict(test_x),squared=False))\n",
    "print()\n",
    "print(\"R-Squared:\")\n",
    "print(\"Training:\")\n",
    "print(r2_score(train_y,lasso.predict(train_x)))\n",
    "print()\n",
    "print(\"Testing:\")\n",
    "print(r2_score(test_y,lasso.predict(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression Cross-Fold Validation\n",
    "\n",
    "    Lasso Regression k-fold cross validation using different alphas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaList = np.logspace(start=-4,stop=0,num=10)\n",
    "alphaList\n",
    "\n",
    "lasso = LassoCV(alphas=alphaList,  \n",
    "                   cv=5,       #Number of folds, i.e., K\n",
    "                   max_iter=5000)\n",
    "lasso.fit(train_x, train_y)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(lasso.alphas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lasso.mse_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np.log10(lasso.alphas_), lasso.mse_path_.mean(axis=1))\n",
    "plt.xlabel('log(alpha)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Lasso Path')\n",
    "plt.axis('tight')\n",
    "plt.show()\n",
    "bestalpha=lasso.alpha_\n",
    "print(bestalpha)\n",
    "print()\n",
    "print(\"Mean Squared Error:\")\n",
    "print(\"Training:\")\n",
    "print(mean_squared_error(train_y,lasso.predict(train_x),squared=False))\n",
    "print()\n",
    "print(\"Testing:\")\n",
    "print(mean_squared_error(test_y,lasso.predict(test_x),squared=False))\n",
    "print()\n",
    "print(\"R-Squared:\")\n",
    "print(\"Training:\")\n",
    "print(r2_score(train_y,lasso.predict(train_x)))\n",
    "print()\n",
    "print(\"Testing:\")\n",
    "print(r2_score(test_y,lasso.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(lasso.coef_))\n",
    "#Check the percentage of non-zero slopes\n",
    "print(np.count_nonzero(lasso.coef_)/len(lasso.coef_))\n",
    "\n",
    "dfbeta = pd.DataFrame({'Term': vectorizer.get_feature_names(),\n",
    "                       'Beta': lasso.coef_\n",
    "                     })\n",
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=False)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbeta.sort_values(by=\"Beta\",inplace=True,ascending=True)\n",
    "dfbeta.reset_index(inplace=True,drop=True)\n",
    "dfbeta.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - K-fold cross Validation\n",
    "    - Used different predictor columns, vectorizers, and N-grams\n",
    "    - Then printed results by predictor columns.\n",
    "    - Only trying to predict if the differences were positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['dayDiff','weekDiff','monthDiff']\n",
    "scorer = ['TF','Binary','NormTfidf','UnnormalizedTfidf']\n",
    "ngrams = [1,2,3]\n",
    "lm = ps.LM()\n",
    "measures = []\n",
    "vector = []\n",
    "grams = []\n",
    "trainAcc = []\n",
    "testAcc = []\n",
    "trainROC = []\n",
    "testROC = []\n",
    "\n",
    "for column in columns:\n",
    "    for score in scorer:\n",
    "        for ngram in ngrams:\n",
    "            if score == 'TF':\n",
    "                vectorizer = CountVectorizer(tokenizer = lm.tokenize,\n",
    "                                             stop_words=custom_stop_words,\n",
    "                                             ngram_range =(ngram,ngram),\n",
    "                                             binary=False)\n",
    "            elif score == 'Binary':\n",
    "                vectorizer = CountVectorizer(tokenizer = lm.tokenize,\n",
    "                                             stop_words=custom_stop_words,\n",
    "                                             ngram_range =(ngram,ngram),\n",
    "                                             binary=True)\n",
    "            elif score == 'NormTfidf':\n",
    "                vectorizer = TfidfVectorizer(tokenizer = lm.tokenize,\n",
    "                                             stop_words=custom_stop_words,\n",
    "                                             norm = 'l1',\n",
    "                                             ngram_range = (ngram,ngram),\n",
    "                                             binary=False)\n",
    "            elif score == 'UnnormalizedTfidf':\n",
    "                vectorizer = TfidfVectorizer(tokenizer = lm.tokenize,\n",
    "                                             stop_words=custom_stop_words,\n",
    "                                             norm = None,\n",
    "                                             ngram_range = (ngram,ngram),\n",
    "                                             binary=False)\n",
    "            \n",
    "            print(f\"Starting: Measure - {column}, Scorer - {score}, N-Grams - {ngram}\")\n",
    "            df = df_all[[\"text\",column]].copy()\n",
    "            df[column] = np.where(df[column]> 0 ,'postive','negative')\n",
    "            df_train, df_test = train_test_split(df, test_size=0.33, random_state=2021)\n",
    "            df_train.reset_index(drop=True,inplace=True)\n",
    "            df_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "            #Create the training DTM and the labels\n",
    "            train_x = vectorizer.fit_transform(df_train[\"text\"])\n",
    "            train_y = df_train[column]\n",
    "            print(train_x.shape)\n",
    "\n",
    "            #Create the testing DTM and the labels\n",
    "            test_x = vectorizer.transform(df_test[\"text\"])\n",
    "            test_y = df_test[column]\n",
    "            print(test_x.shape)\n",
    "\n",
    "            print(pd.DataFrame({'Train': train_y.value_counts(),\n",
    "                      'Test': test_y.value_counts()}))\n",
    "\n",
    "            param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "            print(param_grid)\n",
    "\n",
    "            sparselr = LogisticRegressionCV(penalty='l1', \n",
    "                                        solver='liblinear', \n",
    "                                        Cs=param_grid,   #Use the grid generated above\n",
    "                                        cv=5,            #Number of folds, that is, K\n",
    "                                        scoring='accuracy', #The performance metric to select the best C.\n",
    "                                        random_state=2021,  #To make sure the result is reproducible\n",
    "                                        tol=0.001,\n",
    "                                        max_iter=1000)\n",
    "            sparselr.fit(train_x, train_y)\n",
    "\n",
    "            #All candicates\n",
    "            print(\"All candicates\")\n",
    "            print(sparselr.Cs)\n",
    "            print(\"best value of C among Candidates\")\n",
    "\n",
    "\n",
    "            print(\"Train Accuracy:\")\n",
    "            print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "            print(\"Test Accuracy:\")\n",
    "            print(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "            print(\"Train AUC:\")\n",
    "            print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "            print(\"Test AUC:\")\n",
    "            print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))\n",
    "            \n",
    "            measures.append(column)\n",
    "            vector.append(score)\n",
    "            grams.append(ngram)\n",
    "            trainAcc.append(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "            testAcc.append(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "            trainROC.append(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "            testROC.append(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))\n",
    "            \n",
    "            print(f\"Finishing: Measure - {column}, Scorer - {score}, N-Grams - {ngram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Prediction': measures,\n",
    "                        'Vectorizer': vector,\n",
    "                        'N-Grams': grams,\n",
    "                        'Training Accuracy': trainAcc,\n",
    "                        'Testing Accuracy': testAcc,\n",
    "                        'Training ROC': trainROC,\n",
    "                        'Testing ROC': testROC})\n",
    "results.sort_values(by='Testing ROC',ascending=False,inplace=True)\n",
    "results.reset_index(inplace=True,drop=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_day = results.copy()\n",
    "results_day = results_day[results_day['Prediction'] == 'dayDiff']\n",
    "results_day.sort_values(by='Testing ROC',ascending=False,inplace=True)\n",
    "results_day.reset_index(inplace=True,drop=True)\n",
    "results_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_week = results.copy()\n",
    "results_week = results_week[results_week['Prediction'] == 'weekDiff']\n",
    "results_week.sort_values(by='Testing ROC',ascending=False,inplace=True)\n",
    "results_week.reset_index(inplace=True,drop=True)\n",
    "results_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_month = results.copy()\n",
    "results_month = results_month[results_month['Prediction'] == 'monthDiff']\n",
    "results_month.sort_values(by='Testing ROC',ascending=False,inplace=True)\n",
    "results_month.reset_index(inplace=True,drop=True)\n",
    "results_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in Catapillar Data and Apply Lasso and Logistic Regression Models\n",
    "    - Testing if models developed for Deere and Company can be applied to other companies within the same industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request Data from Edgar Search. Pulled by Company CIK number\n",
    "data = requests.get('https://data.sec.gov/submissions/CIK0000018230.json',headers={'User-Agent':'University of Iowa abromeland@uiowa.edu'})\n",
    "data = json.loads(data.content)['filings']['recent']\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.sort_values(by='filingDate',ascending=False,inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data by accessionNumber and FileName to be added to the dataframe\n",
    "for i in range(len(df)):\n",
    "    accessionNumber = df.loc[i,\"accessionNumber\"]\n",
    "    filename = df.loc[i,'primaryDocument']\n",
    "    URL = f'https://www.sec.gov/Archives/edgar/data/18230/{re.sub(\"-\",\"\",accessionNumber)}/{filename}'\n",
    "    page = requests.get(URL,headers={'User-Agent':'University of Iowa abromeland@uiowa.edu'})\n",
    "    root = html.fromstring(page.content)\n",
    "    text = [s.text_content() for s in root.xpath('/html/body')]\n",
    "    df.loc[i,'text'] = ' '.join(text)\n",
    "    time.sleep(.5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data and print out example data\n",
    "df['text'] = df['text'].str.replace(\"[\\\\u200b||\\\\n||\\\\xa0]\",'',regex=True)\n",
    "df['text'] = df['text'].str.replace(\"\\$\\d+[\\d,\\.]*\",'moneyToken',regex=True)\n",
    "df['text'] = df['text'].str.lower()\n",
    "print(df.filingDate.isnull().sum())\n",
    "print(df.filingDate.dtype)\n",
    "df[\"filingDate\"] = pd.to_datetime(df[\"filingDate\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "print(df.filingDate.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='filingDate',ascending=False,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price = yf.download(\"CAT\",start='2015-08-01',end='2022-07-28',progress=False)\n",
    "print(type(df_price.index))\n",
    "df_price.sort_index(ascending=False,inplace=True)\n",
    "df_price['dayDiff'] = -1*df_price['Adj Close'].diff()\n",
    "df_price['weekDiff'] = -1*df_price['Adj Close'].diff(periods=5)\n",
    "df_price['monthDiff'] = -1*df_price['Adj Close'].diff(periods=22)\n",
    "df_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING - THIS CELL WILL TAKE A LONG TIME TO RUN\n",
    "\n",
    "# AFINN Sentiment Analysis Scores\n",
    "afinn = Afinn(emoticons=True)\n",
    "df[\"AFINN\"]=[afinn.score(s) for s in df.text]\n",
    "\n",
    "# TextBlob Sentiment Analysis Scores\n",
    "df[\"TextBlob\"]=[TextBlob(s).sentiment.polarity for s in df.text]\n",
    "\n",
    "# VADER Sentiment Analysis Scores\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "df[\"VADER\"]=[analyzer.polarity_scores(s)['compound'] for s in df.text]\n",
    "\n",
    "# Loughran and McDonald Sentiment Scores\n",
    "lm = ps.LM()\n",
    "df['LMTitle'] = 0\n",
    "for i in range(len(df['text'])):\n",
    "    tokens = lm.tokenize(df['text'][i])\n",
    "    score = lm.get_score(tokens)\n",
    "    df.loc[i,\"LMTitle\"]=score[\"Polarity\"]\n",
    "\n",
    "df\n",
    "\n",
    "# Scale AFINN score to compare it with other sentiment analyzers\n",
    "df['AFINN_SCALE'] = MinMaxScaler().fit_transform(np.array(df['AFINN']).reshape(-1,1))\n",
    "\n",
    "\n",
    "# inline display of plots\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(df.filingDate, df.AFINN_SCALE.rolling(window=50).mean(), \"-r\", label=\"AFINN Scaled\")\n",
    "plt.plot(df.filingDate, df.TextBlob.rolling(window=50).mean(), \"-b\", label=\"TextBlob\")\n",
    "plt.plot(df.filingDate, df.VADER.rolling(window=50).mean(), \"-g\", label=\"VADER\")\n",
    "plt.plot(df.filingDate, df.LMTitle.rolling(window=50).mean(), \"-k\", label=\"LM\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Comparison on Sentiment Score\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment Score (Moving Average with Window Size 50)\")\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = pd.merge(df,df_price,how='left',left_on = 'filingDate',right_index = True, copy=True)\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph all stock data with Sentiment Data - First Scale difference data to be graphed and compared with sentiment data.\n",
    "df_cat['dayDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_cat['dayDiff']).reshape(-1,1))\n",
    "df_cat['weekDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_cat['weekDiff']).reshape(-1,1))\n",
    "df_cat['monthDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_cat['monthDiff']).reshape(-1,1))\n",
    "df_cat['Adj_Close_SCALE'] = MinMaxScaler().fit_transform(np.array(df_cat['Adj Close']).reshape(-1,1))\n",
    "\n",
    "# Graph Sentiment scores with Adjusted Closing Values\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(df_cat.filingDate, df_cat.AFINN_SCALE.rolling(window=50).mean(), \"-b\", label=\"AFINN Scaled\")\n",
    "plt.plot(df_cat.filingDate, df_cat.TextBlob.rolling(window=50).mean(), \"-g\", label=\"TextBlob\")\n",
    "plt.plot(df_cat.filingDate, df_cat.VADER.rolling(window=50).mean(), \"-r\", label=\"VADER\")\n",
    "plt.plot(df_cat.filingDate, df_cat.LMTitle.rolling(window=50).mean(), \"-k\", label=\"LM\")\n",
    "plt.plot(df_cat.filingDate, df_cat.Adj_Close_SCALE.rolling(window=50).mean(), \"-c\", label=\"Adjusted Close\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Comparison on Sentiment Score\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment Score (Moving Average with Window Size 50)\")\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['characters'] = [len(s) for s in df_cat['text']]\n",
    "print(f\"Average character length: {df_cat['characters'].mean()}\")\n",
    "print(f\"Minimum character length: {df_cat['characters'].min()}\")\n",
    "print(f\"Maximum character length: {df_cat['characters'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_cat[df_cat['characters']<1000].copy()\n",
    "print(df_small.shape)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_cat[df_cat['characters']>500].copy()\n",
    "df_cat.sort_values(by='filingDate',ascending=False,inplace=True)\n",
    "df_cat.reset_index(inplace=True,drop=True)\n",
    "print(df_cat.shape)\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average character length: {df_cat['characters'].mean()}\")\n",
    "print(f\"Minimum character length: {df_cat['characters'].min()}\")\n",
    "print(f\"Maximum character length: {df_cat['characters'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regraining from best Deere Model on entire dataset to be used on Cat Dataset\n",
    "df = df_all[[\"text\",\"Adj Close\"]].copy()\n",
    "\n",
    "\n",
    "lm = ps.LM()\n",
    "vectorizer = TfidfVectorizer(tokenizer = lm.tokenize,stop_words=custom_stop_words)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df[\"text\"])\n",
    "train_y = df[\"Adj Close\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(train_x)\n",
    "train_x=scaler.transform(train_x)\n",
    "\n",
    "lasso = Lasso(alpha=bestalpha, #Regularization parameter.\n",
    "              max_iter=5000\n",
    "              )\n",
    "lasso.fit(train_x,train_y)\n",
    "print(np.count_nonzero(lasso.coef_))\n",
    "#Check the percentage of non-zero slopes\n",
    "print(np.count_nonzero(lasso.coef_)/len(lasso.coef_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cat[['text','Adj Close']]\n",
    "df = df[df['Adj Close'].isna()==False]\n",
    "test_x = vectorizer.transform(df['text'])\n",
    "test_y = df['Adj Close']\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "test_x = scaler.transform(test_x)\n",
    "print(\"Mean Squared Error:\")\n",
    "print(\"Deere:\")\n",
    "print(mean_squared_error(train_y,lasso.predict(train_x),squared=False))\n",
    "print()\n",
    "print(\"Caterpillar:\")\n",
    "print(mean_squared_error(test_y,lasso.predict(test_x),squared=False))\n",
    "print()\n",
    "print(\"R-Squared:\")\n",
    "print(\"Deere:\")\n",
    "print(r2_score(train_y,lasso.predict(train_x)))\n",
    "print()\n",
    "print(\"Caterpillar:\")\n",
    "print(r2_score(test_y,lasso.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_all[[\"text\",\"dayDiff\"]].copy()\n",
    "df_d['dayDiff'] = np.where(df_d.dayDiff > 0 ,'postive','negative')\n",
    "lm = ps.LM()\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,\n",
    "                             stop_words=custom_stop_words,\n",
    "                             ngram_range =(3,3),\n",
    "                             binary=False)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_d[\"text\"])\n",
    "train_y = df_d[\"dayDiff\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "df_c = df_cat[[\"text\",'dayDiff']].copy()\n",
    "df_c['dayDiff'] = np.where(df_c.dayDiff > 0 ,'postive','negative')\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_c[\"text\"])\n",
    "test_y = df_c[\"dayDiff\"]\n",
    "print(test_x.shape)\n",
    "\n",
    "\n",
    "param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "print(param_grid)\n",
    "sparselr = LogisticRegressionCV(penalty='l1', \n",
    "                                solver='liblinear', \n",
    "                                Cs=param_grid,   #Use the grid generated above\n",
    "                                cv=5,            #Number of folds, that is, K\n",
    "                                scoring='accuracy', #The performance metric to select the best C.\n",
    "                                random_state=2021,  #To make sure the result is reproducible\n",
    "                                tol=0.001,\n",
    "                                max_iter=1000)\n",
    "sparselr.fit(train_x, train_y)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Day Difference\")\n",
    "print(\"Accuracy:\")\n",
    "print(\"Deere:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Caterpillar:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "\n",
    "# AUC Score\n",
    "print(\"AUC:\")\n",
    "print(\"Deere:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Caterpillar:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_all[[\"text\",\"weekDiff\"]].copy()\n",
    "df_d['weekDiff'] = np.where(df_d.weekDiff > 0 ,'postive','negative')\n",
    "lm = ps.LM()\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,\n",
    "                             stop_words=custom_stop_words,\n",
    "                             ngram_range =(3,3),\n",
    "                             binary=True)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_d[\"text\"])\n",
    "train_y = df_d[\"weekDiff\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "df_c = df_cat[[\"text\",'weekDiff']].copy()\n",
    "df_c['weekDiff'] = np.where(df_c.weekDiff > 0 ,'postive','negative')\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_c[\"text\"])\n",
    "test_y = df_c[\"weekDiff\"]\n",
    "print(test_x.shape)\n",
    "\n",
    "\n",
    "param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "print(param_grid)\n",
    "sparselr = LogisticRegressionCV(penalty='l1', \n",
    "                                solver='liblinear', \n",
    "                                Cs=param_grid,   #Use the grid generated above\n",
    "                                cv=5,            #Number of folds, that is, K\n",
    "                                scoring='accuracy', #The performance metric to select the best C.\n",
    "                                random_state=2021,  #To make sure the result is reproducible\n",
    "                                tol=0.001,\n",
    "                                max_iter=1000)\n",
    "sparselr.fit(train_x, train_y)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Week Difference\")\n",
    "print(\"Accuracy:\")\n",
    "print(\"Deere:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Caterpillar:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "\n",
    "# AUC Score\n",
    "print(\"AUC:\")\n",
    "print(\"Deere:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Caterpillar:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_all[[\"text\",\"monthDiff\"]].copy()\n",
    "df_d['monthDiff'] = np.where(df_d.monthDiff > 0 ,'postive','negative')\n",
    "lm = ps.LM()\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,\n",
    "                             stop_words=custom_stop_words,\n",
    "                             ngram_range =(2,2),\n",
    "                             binary=True)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_d[\"text\"])\n",
    "train_y = df_d[\"monthDiff\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "df_c = df_cat[[\"text\",'monthDiff']].copy()\n",
    "df_c['monthDiff'] = np.where(df_c.monthDiff > 0 ,'postive','negative')\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_c[\"text\"])\n",
    "test_y = df_c[\"monthDiff\"]\n",
    "print(test_x.shape)\n",
    "\n",
    "\n",
    "param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "print(param_grid)\n",
    "sparselr = LogisticRegressionCV(penalty='l1', \n",
    "                                solver='liblinear', \n",
    "                                Cs=param_grid,   #Use the grid generated above\n",
    "                                cv=5,            #Number of folds, that is, K\n",
    "                                scoring='accuracy', #The performance metric to select the best C.\n",
    "                                random_state=2021,  #To make sure the result is reproducible\n",
    "                                tol=0.001,\n",
    "                                max_iter=1000)\n",
    "sparselr.fit(train_x, train_y)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Month Difference\")\n",
    "print(\"Accuracy:\")\n",
    "print(\"Deere:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Caterpillar:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "\n",
    "# AUC Score\n",
    "print(\"AUC:\")\n",
    "print(\"Deere:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Caterpillar:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in Amazon Data and Apply Lasso and Logistic Regression Models\n",
    "    - Testing if models developed for Deere and Company can be applied to other companies outside of Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request Data from Edgar Search. Pulled by Company CIK number\n",
    "data = requests.get('https://data.sec.gov/submissions/CIK0001018724.json',headers={'User-Agent':'University of Iowa abromeland@uiowa.edu'})\n",
    "data = json.loads(data.content)['filings']['recent']\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.sort_values(by='filingDate',ascending=False,inplace=True)\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data by accessionNumber and FileName to be added to the dataframe\n",
    "for i in range(len(df)):\n",
    "    accessionNumber = df.loc[i,\"accessionNumber\"]\n",
    "    filename = df.loc[i,'primaryDocument']\n",
    "    URL = f'https://www.sec.gov/Archives/edgar/data/1018724/{re.sub(\"-\",\"\",accessionNumber)}/{filename}'\n",
    "    page = requests.get(URL,headers={'User-Agent':'University of Iowa abromeland@uiowa.edu'})\n",
    "    root = html.fromstring(page.content)\n",
    "    text = [s.text_content() for s in root.xpath('/html/body')]\n",
    "    df.loc[i,'text'] = ' '.join(text)\n",
    "    time.sleep(.5)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data and print out example data\n",
    "df['text'] = df['text'].str.replace(\"[\\\\u200b||\\\\n||\\\\xa0]\",'',regex=True)\n",
    "df['text'] = df['text'].str.replace(\"\\$\\d+[\\d,\\.]*\",'moneyToken',regex=True)\n",
    "df['text'] = df['text'].str.lower()\n",
    "print(df.filingDate.isnull().sum())\n",
    "print(df.filingDate.dtype)\n",
    "df[\"filingDate\"] = pd.to_datetime(df[\"filingDate\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "print(df.filingDate.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='filingDate',ascending=False,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price = yf.download(\"AMZN\",start='2013-10-01',end='2022-07-28',progress=False)\n",
    "print(type(df_price.index))\n",
    "df_price.sort_index(ascending=False,inplace=True)\n",
    "df_price['dayDiff'] = -1*df_price['Adj Close'].diff()\n",
    "df_price['weekDiff'] = -1*df_price['Adj Close'].diff(periods=5)\n",
    "df_price['monthDiff'] = -1*df_price['Adj Close'].diff(periods=22)\n",
    "df_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING - THIS CELL WILL TAKE A LONG TIME TO RUN\n",
    "\n",
    "# AFINN Sentiment Analysis Scores\n",
    "afinn = Afinn(emoticons=True)\n",
    "df[\"AFINN\"]=[afinn.score(s) for s in df.text]\n",
    "\n",
    "# TextBlob Sentiment Analysis Scores\n",
    "df[\"TextBlob\"]=[TextBlob(s).sentiment.polarity for s in df.text]\n",
    "\n",
    "# VADER Sentiment Analysis Scores\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "df[\"VADER\"]=[analyzer.polarity_scores(s)['compound'] for s in df.text]\n",
    "\n",
    "# Loughran and McDonald Sentiment Scores\n",
    "lm = ps.LM()\n",
    "df['LMTitle'] = 0\n",
    "for i in range(len(df['text'])):\n",
    "    tokens = lm.tokenize(df['text'][i])\n",
    "    score = lm.get_score(tokens)\n",
    "    df.loc[i,\"LMTitle\"]=score[\"Polarity\"]\n",
    "\n",
    "df\n",
    "\n",
    "# Scale AFINN score to compare it with other sentiment analyzers\n",
    "df['AFINN_SCALE'] = MinMaxScaler().fit_transform(np.array(df['AFINN']).reshape(-1,1))\n",
    "\n",
    "\n",
    "# inline display of plots\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(df.filingDate, df.AFINN_SCALE.rolling(window=50).mean(), \"-r\", label=\"AFINN Scaled\")\n",
    "plt.plot(df.filingDate, df.TextBlob.rolling(window=50).mean(), \"-b\", label=\"TextBlob\")\n",
    "plt.plot(df.filingDate, df.VADER.rolling(window=50).mean(), \"-g\", label=\"VADER\")\n",
    "plt.plot(df.filingDate, df.LMTitle.rolling(window=50).mean(), \"-k\", label=\"LM\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Comparison on Sentiment Score\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment Score (Moving Average with Window Size 50)\")\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = pd.merge(df,df_price,how='left',left_on = 'filingDate',right_index = True, copy=True)\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph all stock data with Sentiment Data - First Scale difference data to be graphed and compared with sentiment data.\n",
    "df_cat['dayDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_cat['dayDiff']).reshape(-1,1))\n",
    "df_cat['weekDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_cat['weekDiff']).reshape(-1,1))\n",
    "df_cat['monthDiff_SCALE'] = MinMaxScaler().fit_transform(np.array(df_cat['monthDiff']).reshape(-1,1))\n",
    "df_cat['Adj_Close_SCALE'] = MinMaxScaler().fit_transform(np.array(df_cat['Adj Close']).reshape(-1,1))\n",
    "\n",
    "# Graph Sentiment scores with Adjusted Closing Values\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(df_cat.filingDate, df_cat.AFINN_SCALE.rolling(window=50).mean(), \"-b\", label=\"AFINN Scaled\")\n",
    "plt.plot(df_cat.filingDate, df_cat.TextBlob.rolling(window=50).mean(), \"-g\", label=\"TextBlob\")\n",
    "plt.plot(df_cat.filingDate, df_cat.VADER.rolling(window=50).mean(), \"-r\", label=\"VADER\")\n",
    "plt.plot(df_cat.filingDate, df_cat.LMTitle.rolling(window=50).mean(), \"-k\", label=\"LM\")\n",
    "plt.plot(df_cat.filingDate, df_cat.Adj_Close_SCALE.rolling(window=50).mean(), \"-c\", label=\"Adjusted Close\")\n",
    "\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Comparison on Sentiment Score\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sentiment Score (Moving Average with Window Size 50)\")\n",
    "plt.grid(axis='both')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['characters'] = [len(s) for s in df_cat['text']]\n",
    "print(f\"Average character length: {df_cat['characters'].mean()}\")\n",
    "print(f\"Minimum character length: {df_cat['characters'].min()}\")\n",
    "print(f\"Maximum character length: {df_cat['characters'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_cat[df_cat['characters']<1000].copy()\n",
    "print(df_small.shape)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_cat[df_cat['characters']>500].copy()\n",
    "df_cat.sort_values(by='filingDate',ascending=False,inplace=True)\n",
    "df_cat.reset_index(inplace=True,drop=True)\n",
    "print(df_cat.shape)\n",
    "df_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average character length: {df_cat['characters'].mean()}\")\n",
    "print(f\"Minimum character length: {df_cat['characters'].min()}\")\n",
    "print(f\"Maximum character length: {df_cat['characters'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regraining from best Deere Model on entire dataset to be used on Cat Dataset\n",
    "df = df_all[[\"text\",\"Adj Close\"]].copy()\n",
    "\n",
    "\n",
    "lm = ps.LM()\n",
    "vectorizer = TfidfVectorizer(tokenizer = lm.tokenize,stop_words=custom_stop_words)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df[\"text\"])\n",
    "train_y = df[\"Adj Close\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(train_x)\n",
    "train_x=scaler.transform(train_x)\n",
    "\n",
    "lasso = Lasso(alpha=bestalpha, #Regularization parameter.\n",
    "              max_iter=5000\n",
    "              )\n",
    "lasso.fit(train_x,train_y)\n",
    "print(np.count_nonzero(lasso.coef_))\n",
    "#Check the percentage of non-zero slopes\n",
    "print(np.count_nonzero(lasso.coef_)/len(lasso.coef_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_cat[['text','Adj Close']]\n",
    "df = df[df['Adj Close'].isna()==False]\n",
    "test_x = vectorizer.transform(df['text'])\n",
    "test_y = df['Adj Close']\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "test_x = scaler.transform(test_x)\n",
    "print(\"Mean Squared Error:\")\n",
    "print(\"Deere:\")\n",
    "print(mean_squared_error(train_y,lasso.predict(train_x),squared=False))\n",
    "print()\n",
    "print(\"Amazon:\")\n",
    "print(mean_squared_error(test_y,lasso.predict(test_x),squared=False))\n",
    "print()\n",
    "print(\"R-Squared:\")\n",
    "print(\"Deere:\")\n",
    "print(r2_score(train_y,lasso.predict(train_x)))\n",
    "print()\n",
    "print(\"Amazon:\")\n",
    "print(r2_score(test_y,lasso.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_all[[\"text\",\"dayDiff\"]].copy()\n",
    "df_d['dayDiff'] = np.where(df_d.dayDiff > 0 ,'postive','negative')\n",
    "lm = ps.LM()\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,\n",
    "                             stop_words=custom_stop_words,\n",
    "                             ngram_range =(3,3),\n",
    "                             binary=False)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_d[\"text\"])\n",
    "train_y = df_d[\"dayDiff\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "df_c = df_cat[[\"text\",'dayDiff']].copy()\n",
    "df_c['dayDiff'] = np.where(df_c.dayDiff > 0 ,'postive','negative')\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_c[\"text\"])\n",
    "test_y = df_c[\"dayDiff\"]\n",
    "print(test_x.shape)\n",
    "\n",
    "\n",
    "param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "print(param_grid)\n",
    "sparselr = LogisticRegressionCV(penalty='l1', \n",
    "                                solver='liblinear', \n",
    "                                Cs=param_grid,   #Use the grid generated above\n",
    "                                cv=5,            #Number of folds, that is, K\n",
    "                                scoring='accuracy', #The performance metric to select the best C.\n",
    "                                random_state=2021,  #To make sure the result is reproducible\n",
    "                                tol=0.001,\n",
    "                                max_iter=1000)\n",
    "sparselr.fit(train_x, train_y)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Day Difference\")\n",
    "print(\"Accuracy:\")\n",
    "print(\"Amazon:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Caterpillar:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "\n",
    "# AUC Score\n",
    "print(\"AUC:\")\n",
    "print(\"Deere:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Amazon:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_all[[\"text\",\"weekDiff\"]].copy()\n",
    "df_d['weekDiff'] = np.where(df_d.weekDiff > 0 ,'postive','negative')\n",
    "lm = ps.LM()\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,\n",
    "                             stop_words=custom_stop_words,\n",
    "                             ngram_range =(3,3),\n",
    "                             binary=True)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_d[\"text\"])\n",
    "train_y = df_d[\"weekDiff\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "df_c = df_cat[[\"text\",'weekDiff']].copy()\n",
    "df_c['weekDiff'] = np.where(df_c.weekDiff > 0 ,'postive','negative')\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_c[\"text\"])\n",
    "test_y = df_c[\"weekDiff\"]\n",
    "print(test_x.shape)\n",
    "\n",
    "\n",
    "param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "print(param_grid)\n",
    "sparselr = LogisticRegressionCV(penalty='l1', \n",
    "                                solver='liblinear', \n",
    "                                Cs=param_grid,   #Use the grid generated above\n",
    "                                cv=5,            #Number of folds, that is, K\n",
    "                                scoring='accuracy', #The performance metric to select the best C.\n",
    "                                random_state=2021,  #To make sure the result is reproducible\n",
    "                                tol=0.001,\n",
    "                                max_iter=1000)\n",
    "sparselr.fit(train_x, train_y)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Week Difference\")\n",
    "print(\"Accuracy:\")\n",
    "print(\"Deere:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Amazon:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "\n",
    "# AUC Score\n",
    "print(\"AUC:\")\n",
    "print(\"Deere:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Amazon:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_all[[\"text\",\"monthDiff\"]].copy()\n",
    "df_d['monthDiff'] = np.where(df_d.monthDiff > 0 ,'postive','negative')\n",
    "lm = ps.LM()\n",
    "vectorizer = CountVectorizer(tokenizer = lm.tokenize,\n",
    "                             stop_words=custom_stop_words,\n",
    "                             ngram_range =(2,2),\n",
    "                             binary=True)\n",
    "#Create the training DTM and the labels\n",
    "train_x = vectorizer.fit_transform(df_d[\"text\"])\n",
    "train_y = df_d[\"monthDiff\"]\n",
    "print(train_x.shape)\n",
    "\n",
    "df_c = df_cat[[\"text\",'monthDiff']].copy()\n",
    "df_c['monthDiff'] = np.where(df_c.monthDiff > 0 ,'postive','negative')\n",
    "#Create the testing DTM and the labels\n",
    "test_x = vectorizer.transform(df_c[\"text\"])\n",
    "test_y = df_c[\"monthDiff\"]\n",
    "print(test_x.shape)\n",
    "\n",
    "\n",
    "param_grid = l1_min_c(train_x, train_y, loss='log') * np.logspace(start=0, stop=5, num=20) \n",
    "print(param_grid)\n",
    "sparselr = LogisticRegressionCV(penalty='l1', \n",
    "                                solver='liblinear', \n",
    "                                Cs=param_grid,   #Use the grid generated above\n",
    "                                cv=5,            #Number of folds, that is, K\n",
    "                                scoring='accuracy', #The performance metric to select the best C.\n",
    "                                random_state=2021,  #To make sure the result is reproducible\n",
    "                                tol=0.001,\n",
    "                                max_iter=1000)\n",
    "sparselr.fit(train_x, train_y)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Month Difference\")\n",
    "print(\"Accuracy:\")\n",
    "print(\"Deere:\")\n",
    "print(accuracy_score(train_y,sparselr.predict(train_x)))\n",
    "print(\"Amazon:\")\n",
    "print(accuracy_score(test_y,sparselr.predict(test_x)))\n",
    "\n",
    "# AUC Score\n",
    "print(\"AUC:\")\n",
    "print(\"Deere:\")\n",
    "print(roc_auc_score(train_y,sparselr.predict_proba(train_x)[:, 1]))\n",
    "print(\"Amazon:\")\n",
    "print(roc_auc_score(test_y,sparselr.predict_proba(test_x)[:, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
